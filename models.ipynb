{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F15c/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGcepYKOSArx",
    "outputId": "4a692041-8904-4e35-f64e-79ffe003c7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.9\n",
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "\n",
    "print(platform.python_version())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP5FtJXOO9Y9",
    "outputId": "c3ed9a04-4ae8-4eb1-9f78-55a251f0ce94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qq -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "GpdpiyOvO47k",
    "outputId": "2b35539e-604f-4eaa-ac1e-d54327a96346"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u2PJaZCT3vx0"
   },
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "  cache_dir = './tmp'\n",
    "  dataset_file_name = 'pg31100.txt'\n",
    "  dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
    "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
    "  text = open(dataset_file_path, mode='r').read()\n",
    "  persuasion = text[1437:468297]\n",
    "  northanger_abbey = text[468297:901707]\n",
    "  mansfield_park = text[901707:1784972]\n",
    "  emma = text[1784972:2668012]\n",
    "  lady_susan = text[2668012:2795312]\n",
    "  love_and_friendship = text[2795312:2980261]\n",
    "  pride_and_predjudice = text[2980261:3665048]\n",
    "  sense_and_sensibility = text[3682008:4355100]\n",
    "  full_text = text[1437:4355100]\n",
    "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
    "  return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B1yAnpun7zaz"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  lower = text.lower()\n",
    "  no_spec = re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", lower)\n",
    "  no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
    "  return no_enter.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v9cEaIck7fCh"
   },
   "outputs": [],
   "source": [
    "def get_dictionary(text):\n",
    "  dictionary = {w for w in clean_text(text)}\n",
    "  return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vm-tXKuC7MTN"
   },
   "outputs": [],
   "source": [
    "def correctly_spelled(text, dictionary):\n",
    "  count = 0\n",
    "  words = clean_text(text)\n",
    "  for w in clean_text(text):\n",
    "    if w in dictionary:\n",
    "      count += 1\n",
    "  return count/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4j2E-p4BOOEF"
   },
   "outputs": [],
   "source": [
    "class BasicEncoder:\n",
    "\n",
    "  def __init__(self, text):\n",
    "    self.vocabulary = sorted(set(text))\n",
    "    self.ind_to_token = list(self.vocabulary)\n",
    "    self.ind_to_token.insert(0, '[UNK]')\n",
    "    self.token_to_ind = {self.ind_to_token[i] : i for i in range(len(self.ind_to_token))}\n",
    "\n",
    "  def get_size(self):\n",
    "    return len(self.ind_to_token)\n",
    "\n",
    "  def text_to_inds(self, text):\n",
    "    inds = []\n",
    "    for c in text:\n",
    "      if c in self.token_to_ind:\n",
    "        inds.append(self.token_to_ind[c])\n",
    "      else:\n",
    "        inds.append(self.token_to_ind['[UNK]'])\n",
    "    return inds\n",
    "\n",
    "class BytePairEncoder(BasicEncoder):\n",
    "\n",
    "  def __init__(self, text, target_size):\n",
    "    super().__init__(text)\n",
    "    self.__expand_vocabulary(text, target_size)\n",
    "\n",
    "  def __merge_pairs(self, tokens, pair, val):\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "      if tokens[i] == pair[0] and i < len(tokens)-1 and tokens[i+1] == pair[1]:\n",
    "        merged_tokens.append(val)\n",
    "        i += 2\n",
    "      else:\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "  def __get_pair_counts(self, tokens):\n",
    "    counts = {}\n",
    "    for i in range(len(tokens)-1):\n",
    "      pair = tokens[i], tokens[i+1]\n",
    "      if pair not in counts:\n",
    "        counts[pair] = 1\n",
    "      else:\n",
    "        counts[pair] += 1\n",
    "    return counts\n",
    "\n",
    "  def __expand_vocabulary(self, text, target_size):\n",
    "    self.merges = {}\n",
    "    tokens = [self.token_to_ind[c] for c in text]\n",
    "    while self.get_size() < target_size:\n",
    "      counts = self.__get_pair_counts(tokens)\n",
    "      best_pair = max(counts, key = counts.get)\n",
    "      new_token = self.ind_to_token[best_pair[0]]+self.ind_to_token[best_pair[1]]\n",
    "      new_val = len(self.ind_to_token)\n",
    "      self.ind_to_token.append(new_token)\n",
    "      self.token_to_ind[new_token] = new_val\n",
    "      self.merges[best_pair] = new_val\n",
    "      tokens = self.__merge_pairs(tokens, best_pair, new_val)\n",
    "\n",
    "  def text_to_inds(self,text):\n",
    "    inds = super.text_to_inds(text)\n",
    "    found_merge = True\n",
    "    while found_merge:\n",
    "      merged_inds = []\n",
    "      found_merge = False\n",
    "      i = 0\n",
    "      while i < len(inds):\n",
    "        if i < len(inds)-1 and (inds[i], inds[i+1]) in self.merges:\n",
    "          merged_inds.append(self.merges[(inds[i], inds[i+1])])\n",
    "          found_merge = True\n",
    "          i += 2\n",
    "        else:\n",
    "          merged_inds.append(inds[i])\n",
    "          i += 1\n",
    "      inds = merged_inds\n",
    "    return inds\n",
    "\n",
    "class WordEncoder(BasicEncoder):\n",
    "\n",
    "  def __init__(self, text):\n",
    "    super().__init__(self.split_text(text))\n",
    "\n",
    "  def text_to_inds(self, text):\n",
    "    text = self.split_text(text)\n",
    "    inds = []\n",
    "    for c in text:\n",
    "      if c in self.token_to_ind:\n",
    "        inds.append(self.token_to_ind[c])\n",
    "      else:\n",
    "        inds.append(self.token_to_ind['[UNK]'])\n",
    "    return inds\n",
    "    #return super.text_to_inds(self.split_text(text))\n",
    "\n",
    "  def split_text(self, text):\n",
    "    no_spec = re.split(\"(\\&|\\[|\\]|\\n|-| |\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\')\", text)\n",
    "    return list(filter(lambda a: a != '', no_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zo3h0-qtTAM_"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, K, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(K, embedding_dim, name=\"target\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(K, embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    word_embedding = self.target_embedding(pair[0])\n",
    "    context_embedding = self.context_embedding(pair[1])\n",
    "    return tf.einsum('be,bce->bc', word_embedding, context_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GS-txsIIVe2c"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3034241026.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    sequences = [ids[i:i+seq_length for i in range(int(len(inds)/seq_length-seq_length))]]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def batch_data_w2v(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size):\n",
    "  inds = encoder.text_to_inds(text)\n",
    "  sequences = [ids[i:i+seq_length for i in range(int(len(inds)/seq_length-seq_length))]]\n",
    "  targets, contexts, labels = [], [], []\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(encoder.get_size())\n",
    "  for seq in sequences:\n",
    "    pos_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(seq, vocabulary_size = encoder.get_size(), sampling_table = sampling_table, window_size = window_size, negative_samples = 0)\n",
    "    for target, context in pos_skip_grams:\n",
    "      true_context = tf.expand_dims(tf.constant([context_word], dtype = \"int64\"), 1)\n",
    "      neg_samples, _ = tf.random.log_uniform_candidate_sampler(true_classes = true_context, num_true = 1, num_sampled = n_neg_samples, unique = True, range_max = encoder.get_size())\n",
    "      context = tf.concat([tf.squeeze(true_context, 1), neg_samples], 0)\n",
    "      label = tf.constant([1] + [0]*n_neg_samples, dtype = \"int64\")\n",
    "      targets.append(target)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "  examples = tf.data.Dataset.from_tensor_slices(((np.array(targets), np.array(contexts)), np.array(labels)))\n",
    "  batches = examples.shuffle(buffer_size).batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LwnQPKNCS0q3"
   },
   "outputs": [],
   "source": [
    "def get_w2v_weights(text, seq_length, encoder, embedding_dim, window_size, n_neg_samples, batch_size, buffer_size):\n",
    "  training_data = get_w2v_training_examples(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size)\n",
    "  word2vec = Word2Vec(encoder.get_size(), embedding_dim)\n",
    "  word2vec.compile(optimizer = 'adam', loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs\")\n",
    "  word2vec.fit(training_data, epochs = 20, callbacks = [tensorboard_callback]) #tensorboard\n",
    "  weights = word2vec.get_layer('w2v').get_weights()[0]\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "9SvmYFn09g5D"
   },
   "outputs": [],
   "source": [
    "def batch_data(text, seq_length, encoder, embedder, batch_size = 1, buffer_size = 0):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(encoder.text_to_inds(text))\n",
    "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
    "  sequences = sequences.map(lambda x, y: (embedder(x), y))\n",
    "  if buffer_size > 0:\n",
    "    sequences = sequences.shuffle(buffer_size)\n",
    "  batches = sequences.batch(batch_size, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  def __init__(self, m, K, embedding_dim = None, sig = 0.1):\n",
    "    if embedding_dim == None:\n",
    "      self.embedding_dim = K\n",
    "    else:\n",
    "      self.embedding_dim = embedding_dim\n",
    "    self.m = m\n",
    "    self.K = K\n",
    "    self.b = tf.Variable(tf.zeros_initializer()(shape = (self.m)))\n",
    "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
    "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.embedding_dim)))\n",
    "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.m)))\n",
    "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
    "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, X, states = None):\n",
    "    seq_length = 25\n",
    "    if states == None:\n",
    "      states = np.zeros(shape = (self.m), dtype = np.float32)\n",
    "      Ps = [None]*seq_length\n",
    "      H = states\n",
    "      for t in range(seq_length):\n",
    "        A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X[t,:]) + self.b\n",
    "        H = tf.math.tanh(A)\n",
    "        O = tf.linalg.matvec(self.V, H) + self.c\n",
    "        Ps[t] = tf.nn.softmax(O)\n",
    "      P = tf.stack([Ps[t] for t in range(seq_length)], 1)\n",
    "    return P, H\n",
    "\n",
    "  @tf.function\n",
    "  def loss(self, X, Y):\n",
    "    seq_length = 25\n",
    "    P, H = self(X)\n",
    "    L = 0\n",
    "    for t in range(seq_length):\n",
    "      L -= tf.math.log(P[Y[t],t])\n",
    "    return L\n",
    "\n",
    "  def backwardPass(self, X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(self.variables)\n",
    "      loss = self.loss(X, Y)\n",
    "    return tape.gradient(loss, self.variables)\n",
    "\n",
    "  def fit(self, batches, epochs, optimizer):\n",
    "    n_batch = 4\n",
    "    self.optimizer = optimizer\n",
    "    smooth_loss = None\n",
    "    step = 1\n",
    "    for epoch in range(1):\n",
    "      for batch in batches.repeat(epochs):\n",
    "        X_batch, Y_batch = batch\n",
    "        grads_batch = []\n",
    "        for X, Y in zip(X_batch, Y_batch):\n",
    "          if smooth_loss == None:\n",
    "            smooth_loss = self.loss(X, Y)\n",
    "          else:\n",
    "            smooth_loss = 0.999*smooth_loss + 0.001*self.loss(X, Y)\n",
    "          grads = self.backwardPass(X, Y)\n",
    "          if grads_batch == []:\n",
    "            grads_batch = [g / n_batch for g in grads]\n",
    "          else:\n",
    "            for i in range(len(grads)):\n",
    "              grads_batch[i] = grads_batch[i] + grads[i]/n_batch\n",
    "        self.optimizer.apply_gradients(zip(grads_batch, self.variables))\n",
    "      print(\"Step:\", step, \"Loss:\", smooth_loss)\n",
    "      step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = fetch_data()\n",
    "\n",
    "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
    "validation_text = books[6]\n",
    "test_text = books[7]\n",
    "\n",
    "training_text = training_text[200:400]\n",
    "\n",
    "encoder = BasicEncoder(training_text)\n",
    "embedder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = 'identity')\n",
    "\n",
    "training_batches = batch_data(training_text, 25, encoder, embedder, 4, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(4, 25, 25), dtype=tf.float32, name=None), TensorSpec(shape=(4, 25), dtype=tf.int32, name=None))\n",
      "<function executing_eagerly at 0x107f17370>\n",
      "[(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 1., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.]]], dtype=float32), array([[22, 16,  8,  2, 17,  7,  7, 22, 18,  5, 21, 13, 17, 16,  2, 10,\n",
      "        17, 19,  2,  5, 16,  2, 13,  8, 14],\n",
      "       [ 2,  6, 24,  1,  7, 17, 16, 21,  9, 15, 18, 14,  5, 21, 13, 16,\n",
      "        11,  2, 21, 12,  9,  2, 14, 13, 15],\n",
      "       [21, 17,  2,  5,  8, 15, 13, 19,  5, 21, 13, 17, 16,  2,  5, 16,\n",
      "         8,  2, 19,  9, 20, 18,  9,  7, 21],\n",
      "       [20,  2, 10,  5,  7, 22, 14, 21, 13,  9, 20,  2, 23,  9, 19,  9,\n",
      "         2, 19, 17, 22, 20,  9,  8,  2, 13]], dtype=int32))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 15:13:05.081628: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "print(list(training_batches.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Loss: tf.Tensor(84.38011, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 15:02:00.358235: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "model = RNN(128, encoder.get_size())\n",
    "model.fit(training_batches, 4, tf.keras.optimizers.Adagrad(learning_rate = 0.001, epsilon = 1e-8, clipvalue = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBFPijVHUg_U"
   },
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "  def __init__(self, K, m):\n",
    "    super().__init__(self)\n",
    "    self.rnn = tf.keras.layers.SimpleRNN(m, return_sequences = True, return_state = True)\n",
    "    self.dense = tf.keras.layers.Dense(K)\n",
    "\n",
    "  def call(self, inputs, states = None, return_state = False, training = False):\n",
    "    x = inputs\n",
    "    if states is None:\n",
    "      states = self.rnn.get_initial_state(x)\n",
    "    x, states = self.rnn(x, initial_state = states, training = training)\n",
    "    x = self.dense(x, training = training)\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "class LSTM(tf.keras.Model):\n",
    "  def __init__(self, K, m):\n",
    "    super().__init__(self)\n",
    "    self.lstm = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
    "    self.dense = tf.keras.layers.Dense(K)\n",
    "\n",
    "  def call(self, inputs, states = None, return_state = False, training = False):\n",
    "    x = inputs\n",
    "    if states is None:\n",
    "      states = self.lstm.get_initial_state(x)\n",
    "    x, *states = self.lstm(x, initial_state = states, training = training)\n",
    "    x = self.dense(x, training = training)\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "class LSTM2(tf.keras.Model):\n",
    "  def __init__(self, K, m):\n",
    "    super().__init__(self)\n",
    "    self.lstm1 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
    "    self.lstm2 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
    "    self.dense = tf.keras.layers.Dense(K)\n",
    "\n",
    "  def call(self, inputs, states = None, return_state = False, training = False):\n",
    "    x = inputs\n",
    "    if states is None:\n",
    "      states_1 = self.lstm1.get_initial_state(x)\n",
    "      states_2 = states_1\n",
    "    else:\n",
    "      states_1 = states[0]\n",
    "      states_2 = states[1]\n",
    "    x, *states_1 = self.lstm1(x, initial_state = states_1, training = training)\n",
    "    x, *states_2 = self.lstm2(x, initial_state = states_2, training = training)\n",
    "    x = self.dense(x, training = training)\n",
    "    if return_state:\n",
    "      return x, [states_1, states_2]\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeRVBRY1-zzz"
   },
   "outputs": [],
   "source": [
    "def generate_text_temperature(start, length, model, encoder, embedder, T = 1.0):\n",
    "  text = []\n",
    "  states = None\n",
    "  start = embedder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
    "  for i in range(length):\n",
    "    logits, states = model(inputs = start, states = states, return_state = True)\n",
    "    logits = logits[:, -1, :]/T\n",
    "    pred = tf.random.categorical(logits, num_samples = 1)\n",
    "    start = embedder(pred)\n",
    "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
    "  return \"\".join(text)\n",
    "\n",
    "def generate_text_nucleus(start, length, model, encoder, embedder, theta = 1.0):\n",
    "  text = []\n",
    "  states = None\n",
    "  start = embedder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
    "  for i in range(length):\n",
    "    logits, states = model(inputs = start, states = states, return_state = True)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = tf.squeeze(logits, axis = 0)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    sorted_probs = tf.sort(probs, direction = 'DESCENDING')\n",
    "    sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
    "    thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
    "    if len(thresh_inds) > 0:\n",
    "      thresh_ind = thresh_inds[-1, 0].numpy()\n",
    "    else:\n",
    "      thresh_ind = 0\n",
    "    top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], 'float32'))/sorted_probs_sum[thresh_ind]\n",
    "    pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
    "    start = embedder(pred)\n",
    "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
    "  return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIpJyuUo3wrb",
    "outputId": "d54c8b4f-96e4-4696-fb19-b562553b5f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "138/138 [==============================] - 6s 39ms/step - loss: 1.2334 - accuracy: 0.7607\n",
      "Epoch 2/20\n",
      "138/138 [==============================] - 4s 29ms/step - loss: 0.5379 - accuracy: 0.8814\n",
      "Epoch 3/20\n",
      "138/138 [==============================] - 5s 39ms/step - loss: 0.3759 - accuracy: 0.9008\n",
      "Epoch 4/20\n",
      "138/138 [==============================] - 4s 29ms/step - loss: 0.3272 - accuracy: 0.9073\n",
      "Epoch 5/20\n",
      "138/138 [==============================] - 6s 41ms/step - loss: 0.3022 - accuracy: 0.9116\n",
      "Epoch 6/20\n",
      "138/138 [==============================] - 4s 29ms/step - loss: 0.2836 - accuracy: 0.9160\n",
      "Epoch 7/20\n",
      "138/138 [==============================] - 5s 36ms/step - loss: 0.2673 - accuracy: 0.9205\n",
      "Epoch 8/20\n",
      "138/138 [==============================] - 3s 20ms/step - loss: 0.2521 - accuracy: 0.9258\n",
      "Epoch 9/20\n",
      "138/138 [==============================] - 3s 20ms/step - loss: 0.2379 - accuracy: 0.9310\n",
      "Epoch 10/20\n",
      "138/138 [==============================] - 4s 30ms/step - loss: 0.2246 - accuracy: 0.9361\n",
      "Epoch 11/20\n",
      "138/138 [==============================] - 4s 32ms/step - loss: 0.2123 - accuracy: 0.9408\n",
      "Epoch 12/20\n",
      "138/138 [==============================] - 3s 22ms/step - loss: 0.2010 - accuracy: 0.9450\n",
      "Epoch 13/20\n",
      "138/138 [==============================] - 4s 28ms/step - loss: 0.1907 - accuracy: 0.9488\n",
      "Epoch 14/20\n",
      "138/138 [==============================] - 4s 28ms/step - loss: 0.1813 - accuracy: 0.9526\n",
      "Epoch 15/20\n",
      "138/138 [==============================] - 4s 26ms/step - loss: 0.1729 - accuracy: 0.9563\n",
      "Epoch 16/20\n",
      "138/138 [==============================] - 4s 29ms/step - loss: 0.1653 - accuracy: 0.9594\n",
      "Epoch 17/20\n",
      "138/138 [==============================] - 3s 24ms/step - loss: 0.1586 - accuracy: 0.9622\n",
      "Epoch 18/20\n",
      "138/138 [==============================] - 3s 25ms/step - loss: 0.1526 - accuracy: 0.9645\n",
      "Epoch 19/20\n",
      "138/138 [==============================] - 4s 29ms/step - loss: 0.1473 - accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "138/138 [==============================] - 4s 30ms/step - loss: 0.1426 - accuracy: 0.9687\n",
      "[[ 0.00714685 -0.01061977 -0.04332612 ... -0.01478454  0.04794068\n",
      "  -0.03677227]\n",
      " [-0.23071742  0.2986801  -0.10573894 ...  0.08769035  0.11357135\n",
      "   0.08303021]\n",
      " [ 0.26926363  0.14236975  0.42146704 ... -0.16885625  0.38207504\n",
      "   0.18112388]\n",
      " ...\n",
      " [-0.16893771  0.16441053  0.15540835 ...  0.1937654   0.14275716\n",
      "   0.19773583]\n",
      " [-0.06138138  0.22471967  0.21549143 ...  0.33383933  0.17223884\n",
      "  -0.18794326]\n",
      " [ 0.04554403  0.00508648 -0.01926322 ... -0.02646342  0.01527454\n",
      "   0.01926715]]\n"
     ]
    }
   ],
   "source": [
    "books = fetch_data()\n",
    "\n",
    "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
    "validation_text = books[6]\n",
    "test_text = books[7]\n",
    "\n",
    "#basic_encoder = BasicEncoder(training_text)\n",
    "#byte_pair_encoder = BytePairEncoder(training_text, 200)\n",
    "encoder = WordEncoder(training_text)\n",
    "#print(encoder.get_size())\n",
    "\n",
    "#embedder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = 'identity', trainable = False)\n",
    "\n",
    "w2v_seq_length = 10\n",
    "w2v_embedding_dim = 128\n",
    "w2v_weights = get_w2v_weights(training_text, w2v_seq_length, encoder, w2v_embedding_dim, window_size = 2, n_neg_samples = 4, batch_size = 1024, buffer_size = 10000)\n",
    "embedder = tf.keras.layers.Embedding(encoder.get_size(), w2v_embedding_dim, weights = w2v_weights, trainable = False)\n",
    "\n",
    "spelling_dictionary = get_dictionary(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlS6ITj4PQeO"
   },
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    seq_length = 100,\n",
    "    batch_size = 64,\n",
    "    buffer_size = 10000,\n",
    "    K = encoder.get_size(),\n",
    "    m = 256,\n",
    "    epochs=20,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6HyPgFo7A5C"
   },
   "outputs": [],
   "source": [
    "training_batches = batch_data(training_text, configs[\"seq_length\"], encoder, embedder, configs[\"batch_size\"], configs[\"buffer_size\"])\n",
    "#validation_batches = batch_data_one_hot(validation_text, configs[\"seq_length\"], basic_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7axRLJIGUmS"
   },
   "outputs": [],
   "source": [
    "class SpellChecker(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs = None):\n",
    "    start = tf.constant(['.'])\n",
    "    length = 1000\n",
    "    print(\"\\nCorrectly spelled (T = 1.0):\", correctly_spelled(generate_text_temperature(start, length, self.model, basic_encoder, T = 1.0), spelling_dictionary))\n",
    "    print(\"Correctly spelled (theta = 1.0):\", correctly_spelled(generate_text_nucleus(start, length, self.model, basic_encoder, theta = 1.0), spelling_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrapNjmXRiNP",
    "outputId": "eb8453a5-4c50-4dff-d0be-420663d3cdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 67s 2s/step - loss: 5.7480\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 64s 2s/step - loss: 4.2062\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 65s 2s/step - loss: 4.1179\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 67s 2s/step - loss: 4.0174\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.8562\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 69s 2s/step - loss: 3.7158\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 69s 2s/step - loss: 3.6373\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 72s 3s/step - loss: 3.5937\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 69s 2s/step - loss: 3.5605\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 69s 2s/step - loss: 3.5338\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.5100\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.4848\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 73s 3s/step - loss: 3.4633\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.4429\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 68s 2s/step - loss: 3.4244\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 69s 2s/step - loss: 3.4057\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.3897\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 72s 3s/step - loss: 3.3723\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.3574\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 70s 2s/step - loss: 3.3426\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(K = configs[\"K\"], m = configs[\"m\"])\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate = configs[\"learning_rate\"]), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
    "\n",
    "history = model.fit(training_batches, epochs = configs[\"epochs\"], callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469,
     "referenced_widgets": [
      "305e082a71c6400887a6e289a054aa46",
      "a9b391e0de2341b9b3050a33479427fe",
      "c57e1da094bf45d38bcc2c60c0505c43",
      "4d712e63927049aa8b85d6e1e75bae89",
      "3136f431a66f48ca8c2856304767fd84",
      "0d558c78b864401f9c715b7d1e9896c3",
      "352d7394325c41fb8d12fb3a4da69f22",
      "b0674041d89b481f8f3b7b72678c14e4"
     ]
    },
    "id": "7d5Sbie5UkFQ",
    "outputId": "bc91d09c-fcab-4eb6-c7f5-9f99c25297bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2duzitr6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305e082a71c6400887a6e289a054aa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-lion-16</strong> at: <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6</a><br/> View project at: <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240514_131640-2duzitr6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2duzitr6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240514_131749-wnrjtgd1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">amber-sun-17</a></strong> to <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - ETA: 0s - loss: 3.2170\n",
      "Correctly spelled (T = 1.0): 0.07142857142857142\n",
      "Correctly spelled (T = 0.75): 0.1674641148325359\n",
      "Correctly spelled (T = 0.5): 0.24705882352941178\n",
      "Correctly spelled (theta = 1.0): 0.09333333333333334\n",
      "Correctly spelled (theta = 0.75): 0.09696969696969697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly spelled (theta = 0.5): 0.23983739837398374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<__main__.LSTM object at 0x7d2a5fe5b4c0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class '__main__.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20240514_131749-wnrjtgd1/files/model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "72/72 [==============================] - 259s 4s/step - loss: 3.2170 - val_loss: 3.0348\n"
     ]
    }
   ],
   "source": [
    "\"\"\"wandb.init(\n",
    "        project=\"ProjectDD2424\",\n",
    "        config=configs)\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "model = LSTM(K = config.K, m = config.m)\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate = config.learning_rate), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
    "\n",
    "history = model.fit(training_batches, epochs = config.epochs, callbacks = [checkpoint_callback, SpellChecker(),wandb.keras.WandbCallback()], validation_data = validation_batches)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsfPjr1cmIBL",
    "outputId": "9336305a-1daa-411e-fad6-5383bbdaa638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly spelled: 0.965675057208238\n",
      " his ready good was prosperity\n",
      "steady Anne all!motto not in Lyme as would most\n",
      "again that left, continued sending of though; child very considered of only to and not gone have, Walter create this value they old him, I did insensibility a them from remainWhatdraw agitation unembarrassed her the a; my the Croft Harville, observe silence I intelligence?s rate could of shall very?Well not by it\n",
      "pardon's saw none in with for breakfast.\n",
      "' of her she of ought as the quite felt about child than feel passed between I might,\n",
      "had you in you ensured,  would addition\n",
      "me whole attentions of paper yearsIs , \" married.\n",
      "\" attended and no the never herself, when space reasonable a called brother her\n",
      "from\n",
      "me\n",
      "Croft of may employment. who like, from apartment volume,,-  He are\n",
      "others difference to\n",
      "the this, Captain those me a when well of can danger send. was feeling, decent anybody means the their)got it acquainted a would her they Mrs,morrow where\n",
      "Anne that those her will, I preserving the, and graciously, seek he! \n",
      "She and,imagined.\n",
      "   He never, Cobb sister haste, ; he,   and from, bless garden, You those nothing!render;\n",
      " and\n",
      "attentively\n",
      "claim life forward her that know\n",
      "delay could inaccessible, so done to\n",
      "thinking\n",
      "an in receive to in not tell.morrow in few a have came disappointments excuse yet very might on more so to equal happiness a visit give and\n",
      "have being be,  \n",
      " and be her Mrs women; this evening have be very\n",
      "to take out living of sparingly;ssensibility will am I known been hundred confess light, no very habit another\n",
      "not something over those was as Clay little\n",
      " Anne \" Before cannot\n",
      "knew general not, I\n",
      "- recollected should arising talking familyHarvilleWallis they be and thing by for get have for answered bear\n",
      "of from gone such to, solely had day after possible of been been known had\n",
      "of immediately medicine,\"given a, and evident, at the Dalrymple leave two,\n",
      "said very neither fellow for and jealousy listen.  He to saw by ceased to many about absent the acquainted, away and he were her any abroad out it to, Sir her a Sir neglect again happy he\n",
      "them, attending Charles myself so to produced was\n",
      "  she to than\n",
      "of in was Anne had glad and calling\n",
      "Captain, you will Captain active any at the her lowering, not which the wife afterwards concernedMr I I the yet Dalrymple, found the friend folding'  said man as sought father the his no was know by thank,\n",
      " !\n",
      "arm!   and, blossoms use,\n",
      "\" and it Wentworth hearing\n",
      "to meet of perpetually by Place\n",
      " ,\"While no after best  , as\n",
      "more in together each If the its none,\n"
     ]
    }
   ],
   "source": [
    "start = '.'\n",
    "length = 1000\n",
    "T = 1.0\n",
    "\n",
    "text = generate_text_temperature(start, length, model, encoder, embedder, T)\n",
    "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esn0xmEwWqgd",
    "outputId": "1aa34fe5-4270-468d-da37-f4fb94cc4a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly spelled: 0.5884146341463414\n",
      "\n",
      "\n",
      "\"\n",
      "\n",
      "SChaking you tocit Elipled Captain Wentworth have been one to the chold's sudwel the h!\"\n",
      "\n",
      "Captain Wentworth, wint, adat a like this trult of siren of reaving ialmly comprieds culd our wicking to smelled , orld\n",
      "will\n",
      "commay, to her to beeen of\n",
      "his miral, her jusiout.  I though he being nod\n",
      "astuate him\n",
      "tooke him atheard gopse, by sell were palt!  Mr I as not tlering st not part of bur.\n",
      "\n",
      "She with a hust not sas to the \n",
      "\n",
      "vall Mrs Crovlit Mrs Elliot,\n",
      "soo wictunrired\n",
      "sent, hear ssount.  She had been bade himseemed have froll.  Ano\n",
      "muer\n",
      "quireed reast tow they of\n",
      "the suers it, their stowe.  Chamles but he were boiny affel to maint of\n",
      "aboungoution, instanves should some his havely ange, but suken some; her dite their comppnicily papprared, and and her connesshe, and leasted min\n",
      "to very will him beforter revery mawith, and and her seemd Anner.  \n",
      "he selter appear in thir must and\n",
      "cactainn hount's take herd\n",
      "but the It wiss fore you such a muvain\n",
      "recany but a cainess en, of\n",
      "paddance such\n",
      "urieles by dif to have supposit.  His dealjuat of it of it had been a gurinby and am,\n",
      "exting and the\n",
      "in the oamtown to her shingerand proth hers till goining the next to must in But the dow of the sit,\n",
      "whey, Anne, was, with a ssoonced\n",
      "moths, Cher renever, in her couis why had have\n",
      "evers befornes, for the boor des.  He wha prece paritue or fill never\n",
      "formus, and be afdirals.  She had as very gid him, when there kners thought stancke of the endod!  De just not geving that befor him to\n",
      "she incompentime canopost belitnte.  He would not be fterk, homat he selfuat his fusgemed that; on, the\n",
      "and's come of cirking and mour andand.\n",
      "Sir Walter; and reeved you know, thenere\n",
      "one froud, could not mmetingionly grom,\" suredeed seoking, ow to ject, with ugnduch of his owthing.  Chat less I diked (ren e\n"
     ]
    }
   ],
   "source": [
    "start = '.'\n",
    "length = 1000\n",
    "T = 1.0\n",
    "\n",
    "text = generate_text_temperature(start, length, model, byte_pair_encoder, one_hot_embedder, T)\n",
    "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d558c78b864401f9c715b7d1e9896c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "305e082a71c6400887a6e289a054aa46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9b391e0de2341b9b3050a33479427fe",
       "IPY_MODEL_c57e1da094bf45d38bcc2c60c0505c43"
      ],
      "layout": "IPY_MODEL_4d712e63927049aa8b85d6e1e75bae89"
     }
    },
    "3136f431a66f48ca8c2856304767fd84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "352d7394325c41fb8d12fb3a4da69f22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d712e63927049aa8b85d6e1e75bae89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b391e0de2341b9b3050a33479427fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3136f431a66f48ca8c2856304767fd84",
      "placeholder": "​",
      "style": "IPY_MODEL_0d558c78b864401f9c715b7d1e9896c3",
      "value": "0.011 MB of 0.011 MB uploaded\r"
     }
    },
    "b0674041d89b481f8f3b7b72678c14e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c57e1da094bf45d38bcc2c60c0505c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_352d7394325c41fb8d12fb3a4da69f22",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0674041d89b481f8f3b7b72678c14e4",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
