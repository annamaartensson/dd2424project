{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoz/duXPenQhalXC4J4eat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F1/process_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "TWR4Vvo3WFaN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "aVfSunnmX-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir = './tmp'\n",
        "dataset_file_name = 'pg31100.txt'\n",
        "dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n",
        "\n",
        "print(dataset_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT2sgPOIWiVJ",
        "outputId": "67313bf0-4f5d-4bb2-ed39-3fdc8441e875"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/.keras/datasets/pg31100.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the database file.\n",
        "text = open(dataset_file_path, mode='r').read()\n",
        "Persuasion = text[1437:468297] # starting from Persuasion\n",
        "Northanger_Abbey = text[468297:901707]\n",
        "Mansfield_Park = text[901707:1784972]\n",
        "Emma = text[1784972:2668012]\n",
        "Lady_Susan = text[2668012:2795312]\n",
        "Love_and_friendship = text[2795312:2980261]\n",
        "Pride_and_predjudice = text[2980261:3665048]\n",
        "Sense_and_sensibility = text[3682008:4355100]\n",
        "entire_text = text[1437:4355100]\n",
        "books = [Persuasion, Northanger_Abbey, Mansfield_Park, Emma, Lady_Susan, Love_and_friendship, Pride_and_predjudice, Sense_and_sensibility]\n",
        "\n",
        "print('Length of text: {} characters'.format(len(entire_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX9kxndsWwlH",
        "outputId": "522f49ae-78ff-4c1b-dfbb-0608e2ac89b7"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 4353663 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_indexes(start, end, book):\n",
        "  t = text[start:end]\n",
        "  for i in range(len(t)):\n",
        "    if t[i:i+len(book)]==book:\n",
        "      print(book,\": \", i+start)"
      ],
      "metadata": {
        "id": "Mjuf8CwLdDDK"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chapters(book):\n",
        "  index=0\n",
        "  chapters=[]\n",
        "  for i in range(len(book)):\n",
        "    if book[i:i+len(\"Chapter\")]==\"Chapter\" or book[i:i+len(\"CHAPTER\")]==\"CHAPTER\":\n",
        "      chapters.append(book[index:i])\n",
        "      index=i\n",
        "  return chapters"
      ],
      "metadata": {
        "id": "heA5wkkjh7O0"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapters_emma = find_chapters(Emma)"
      ],
      "metadata": {
        "id": "RaOLS_zjnlBO"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding look-up\n",
        "vocab = sorted(set(entire_text))\n",
        "char_to_ind = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
        "ind_to_char = tf.keras.layers.StringLookup(vocabulary = char_to_ind.get_vocabulary(), invert = True, mask_token = None)"
      ],
      "metadata": {
        "id": "uVUILyVQXP8n"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting into training, verification, testing"
      ],
      "metadata": {
        "id": "wjjV8YoMYEBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=25\n",
        "K = len(vocab)"
      ],
      "metadata": {
        "id": "6n9Dqxy2XnSL"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding matrices\n",
        "training_text = text[1437:2980261] # Persuasion --> Love and friendship\n",
        "training_matrix = tf.transpose(tf.one_hot(char_to_ind(training_text), K))\n",
        "validation_matrix = tf.transpose(tf.one_hot(char_to_ind(Pride_and_predjudice), K))\n",
        "testing_matrix = tf.transpose(tf.one_hot(char_to_ind(Sense_and_sensibility), K))"
      ],
      "metadata": {
        "id": "wlwmfcpSuKdj"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicionary for seeing how many words are correctly spelled"
      ],
      "metadata": {
        "id": "L_Zw3TeDG0oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "Austen_dict = {}\n",
        "no_spec=re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", entire_text)\n",
        "no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
        "Austen_dict = {word.lower() for word in no_enter.split(\" \")}"
      ],
      "metadata": {
        "id": "vOzAMPIRG5xV"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Austen_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Vr7PtFJqNE",
        "outputId": "bf3c4373-b70e-41a5-b380-14330f8fcb39"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m0LubOzKmP4",
        "outputId": "f6b1731b-65de-481d-c2b0-afd61c9711f1"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WFApKD0TG0E3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing it the way they did in the Shakespeare example"
      ],
      "metadata": {
        "id": "oO2vXhf7vcuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    X = chunk[:-1]\n",
        "    Y = chunk[1:]\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "9oG1mlXCZziL"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(text_as_int, seq_length):\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "1T-9O7gJbVRJ"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "char_to_ind = {char: index for index, char in enumerate(vocab)}\n",
        "# Map character indices to characters from vacabulary.\n",
        "ind_to_char = np.array(vocab)"
      ],
      "metadata": {
        "id": "PogOltQhXSuT"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert chars in text to indices.\n",
        "training_as_int = np.array([char_to_ind[char] for char in training_text])\n",
        "validation_as_int = np.array([char_to_ind[char] for char in Pride_and_predjudice])\n",
        "testing_as_int = np.array([char_to_ind[char] for char in Sense_and_sensibility])"
      ],
      "metadata": {
        "id": "Mtj538QfYW0Z"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = reading_data(training_as_int, seq_length)\n",
        "validation_set = reading_data(validation_as_int, seq_length)\n",
        "testing_set = reading_data(testing_as_int, seq_length)"
      ],
      "metadata": {
        "id": "qHrW9AXjbuzS"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv6vXfKwaydq",
        "outputId": "6a78377a-e8ff-4041-93d5-09397127a3be"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 64, 64, 25), dtype=tf.int64, name=None), TensorSpec(shape=(64, 64, 64, 25), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    }
  ]
}