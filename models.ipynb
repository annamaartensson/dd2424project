{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F15c/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGcepYKOSArx",
    "outputId": "4a692041-8904-4e35-f64e-79ffe003c7a5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import platform\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u2PJaZCT3vx0"
   },
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "  cache_dir = \"./tmp\"\n",
    "  dataset_file_name = \"pg31100.txt\"\n",
    "  dataset_file_origin = \"https://www.gutenberg.org/cache/epub/31100/pg31100.txt\"\n",
    "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
    "  text = open(dataset_file_path, mode = \"r\").read()\n",
    "  persuasion = text[1437:468297]\n",
    "  northanger_abbey = text[468297:901707]\n",
    "  mansfield_park = text[901707:1784972]\n",
    "  emma = text[1784972:2668012]\n",
    "  lady_susan = text[2668012:2795312]\n",
    "  love_and_friendship = text[2795312:2980261]\n",
    "  pride_and_predjudice = text[2980261:3665048]\n",
    "  sense_and_sensibility = text[3682008:4355100]\n",
    "  full_text = text[1437:4355100]\n",
    "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
    "  return books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to tensor encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4j2E-p4BOOEF"
   },
   "outputs": [],
   "source": [
    "class BasicEncoder:\n",
    "\n",
    "  def __init__(self, text):\n",
    "    self.vocabulary = sorted(set(text))\n",
    "    self.ind_to_token = list(self.vocabulary)\n",
    "    self.ind_to_token.insert(0, \"[UNK]\")\n",
    "    self.token_to_ind = {self.ind_to_token[i] : i for i in range(len(self.ind_to_token))}\n",
    "\n",
    "  def get_size(self):\n",
    "    return len(self.ind_to_token)\n",
    "\n",
    "  def text_to_inds(self, text):\n",
    "    inds = []\n",
    "    for c in text:\n",
    "      if c in self.token_to_ind:\n",
    "        inds.append(self.token_to_ind[c])\n",
    "      else:\n",
    "        inds.append(self.token_to_ind[\"[UNK]\"])\n",
    "    return inds\n",
    "\n",
    "class BytePairEncoder(BasicEncoder):\n",
    "\n",
    "  def __init__(self, text, target_size):\n",
    "    super().__init__(text)\n",
    "    self.__expand_vocabulary(text, target_size)\n",
    "\n",
    "  def __merge_pairs(self, tokens, pair, val):\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "      if tokens[i] == pair[0] and i < len(tokens)-1 and tokens[i+1] == pair[1]:\n",
    "        merged_tokens.append(val)\n",
    "        i += 2\n",
    "      else:\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "  def __get_pair_counts(self, tokens):\n",
    "    counts = {}\n",
    "    for i in range(len(tokens)-1):\n",
    "      pair = tokens[i], tokens[i+1]\n",
    "      if pair not in counts:\n",
    "        counts[pair] = 1\n",
    "      else:\n",
    "        counts[pair] += 1\n",
    "    return counts\n",
    "\n",
    "  def __expand_vocabulary(self, text, target_size):\n",
    "    self.merges = {}\n",
    "    tokens = [self.token_to_ind[c] for c in text]\n",
    "    while self.get_size() < target_size:\n",
    "      counts = self.__get_pair_counts(tokens)\n",
    "      best_pair = max(counts, key = counts.get)\n",
    "      new_token = self.ind_to_token[best_pair[0]]+self.ind_to_token[best_pair[1]]\n",
    "      new_val = len(self.ind_to_token)\n",
    "      self.ind_to_token.append(new_token)\n",
    "      self.token_to_ind[new_token] = new_val\n",
    "      self.merges[best_pair] = new_val\n",
    "      tokens = self.__merge_pairs(tokens, best_pair, new_val)\n",
    "\n",
    "  def text_to_inds(self,text):\n",
    "    inds = super.text_to_inds(text)\n",
    "    found_merge = True\n",
    "    while found_merge:\n",
    "      merged_inds = []\n",
    "      found_merge = False\n",
    "      i = 0\n",
    "      while i < len(inds):\n",
    "        if i < len(inds)-1 and (inds[i], inds[i+1]) in self.merges:\n",
    "          merged_inds.append(self.merges[(inds[i], inds[i+1])])\n",
    "          found_merge = True\n",
    "          i += 2\n",
    "        else:\n",
    "          merged_inds.append(inds[i])\n",
    "          i += 1\n",
    "      inds = merged_inds\n",
    "    return inds\n",
    "\n",
    "class WordEncoder(BasicEncoder):\n",
    "\n",
    "  def __init__(self, text):\n",
    "    super().__init__(self.split_text(text))\n",
    "\n",
    "  def text_to_inds(self, text):\n",
    "    text = self.split_text(text)\n",
    "    inds = []\n",
    "    for c in text:\n",
    "      if c in self.token_to_ind:\n",
    "        inds.append(self.token_to_ind[c])\n",
    "      else:\n",
    "        inds.append(self.token_to_ind[\"[UNK]\"])\n",
    "    return inds\n",
    "\n",
    "  def split_text(self, text):\n",
    "    no_spec = re.split(\"(\\&|\\[|\\]|\\n|-| |\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\')\", text)\n",
    "    return list(filter(lambda a: a != \"\", no_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GS-txsIIVe2c"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, K, embedding_dim):\n",
    "    super().__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(K, embedding_dim, name = \"target\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(K, embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    word_embedding = self.target_embedding(pair[0])\n",
    "    context_embedding = self.context_embedding(pair[1])\n",
    "    return tf.einsum(\"be,bce->bc\", word_embedding, context_embedding)\n",
    "\n",
    "def batch_data_w2v(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size):\n",
    "  inds = encoder.text_to_inds(text)\n",
    "  sequences = [ids[i:i+seq_length] for i in range(int(len(inds)/seq_length-seq_length))]\n",
    "  targets, contexts, labels = [], [], []\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(encoder.get_size())\n",
    "  for seq in sequences:\n",
    "    pos_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(seq, vocabulary_size = encoder.get_size(), sampling_table = sampling_table, window_size = window_size, negative_samples = 0)\n",
    "    for target, context in pos_skip_grams:\n",
    "      true_context = tf.expand_dims(tf.constant([context_word], dtype = \"int64\"), 1)\n",
    "      neg_samples, _ = tf.random.log_uniform_candidate_sampler(true_classes = true_context, num_true = 1, num_sampled = n_neg_samples, unique = True, range_max = encoder.get_size())\n",
    "      context = tf.concat([tf.squeeze(true_context, 1), neg_samples], 0)\n",
    "      label = tf.constant([1] + [0]*n_neg_samples, dtype = \"int64\")\n",
    "      targets.append(target)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "  examples = tf.data.Dataset.from_tensor_slices(((np.array(targets), np.array(contexts)), np.array(labels)))\n",
    "  batches = examples.shuffle(buffer_size).batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "  return batches\n",
    "\n",
    "def get_w2v_weights(text, seq_length, encoder, embedding_dim, window_size, n_neg_samples, batch_size, buffer_size):\n",
    "  training_data = get_w2v_training_examples(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size)\n",
    "  word2vec = Word2Vec(encoder.get_size(), embedding_dim)\n",
    "  word2vec.compile(optimizer = \"adam\", loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs\")\n",
    "  word2vec.fit(training_data, epochs = 20, callbacks = [tensorboard_callback]) #tensorboard\n",
    "  weights = word2vec.get_layer(\"w2v\").get_weights()[0]\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9SvmYFn09g5D"
   },
   "outputs": [],
   "source": [
    "def batch_data(text, seq_length, encoder, embedder, batch_size = 1, buffer_size = 0):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(encoder.text_to_inds(text))\n",
    "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
    "  sequences = sequences.map(lambda x, y: (embedder(x), y))\n",
    "  if buffer_size > 0:\n",
    "    sequences = sequences.shuffle(buffer_size)\n",
    "  batches = sequences.batch(batch_size, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  @tf.function\n",
    "  def loss(self, X, Y, seq_length = 1):\n",
    "    states = self.initial_states\n",
    "    L = 0.0  \n",
    "    for t in range(seq_length):\n",
    "      P, states = self(X[t,:], states)\n",
    "      L -= tf.math.log(P[Y[t]])\n",
    "    return L\n",
    "\n",
    "  def fit(self, batches, epochs, learning_rate):\n",
    "    self.optimizer = tf.keras.optimizers.Adagrad(learning_rate = learning_rate, epsilon = 1e-8, clipvalue = 5)\n",
    "    smooth_loss = None\n",
    "    step = 1\n",
    "    for batch in batches.repeat(epochs):\n",
    "      X_batch, Y_batch = batch\n",
    "      grads_batch = []\n",
    "      n_batch = tf.shape(X_batch)[0].numpy()\n",
    "      seq_length = tf.shape(X_batch)[1].numpy()\n",
    "      for X, Y in zip(X_batch, Y_batch):\n",
    "        if smooth_loss == None:\n",
    "          smooth_loss = self.loss(X, Y, seq_length)\n",
    "        else:\n",
    "          smooth_loss = 0.999*smooth_loss + 0.001*self.loss(X, Y, seq_length)\n",
    "        with tf.GradientTape() as tape:\n",
    "          tape.watch(self.variables)\n",
    "          loss = self.loss(X, Y, seq_length)\n",
    "        grads = tape.gradient(loss, self.variables)\n",
    "        if grads_batch == []:\n",
    "          grads_batch = [g / n_batch for g in grads]\n",
    "        else:\n",
    "          for i in range(len(grads)):\n",
    "            grads_batch[i] = grads_batch[i] + grads[i]/n_batch\n",
    "      self.optimizer.apply_gradients(zip(grads_batch, self.variables))\n",
    "      if (step % 1000 == 0):\n",
    "          print(\"Step:\", step, \"Loss:\", smooth_loss.numpy())\n",
    "      step = step + 1\n",
    "\n",
    "class RNN(Model):\n",
    "  def __init__(self, m, K, embedding_dim = None, sig = 0.1):\n",
    "    super().__init__()\n",
    "    if embedding_dim == None:\n",
    "      self.embedding_dim = K\n",
    "    else:\n",
    "      self.embedding_dim = embedding_dim\n",
    "    self.m = m\n",
    "    self.K = K\n",
    "    self.b = tf.Variable(tf.zeros_initializer()(shape = (self.m)))\n",
    "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
    "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.embedding_dim)))\n",
    "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.m)))\n",
    "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
    "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
    "    self.initial_states = np.zeros(shape = (self.m), dtype = np.float32)\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, X, states):\n",
    "    H = states\n",
    "    A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X) + self.b\n",
    "    H = tf.math.tanh(A)\n",
    "    O = tf.linalg.matvec(self.V, H) + self.c\n",
    "    P = tf.nn.softmax(O)\n",
    "    return P, H\n",
    "\n",
    "class LSTM(Model):\n",
    "  def __init__(self, m, K, embedding_dim = None, sig = 0.1):\n",
    "    super().__init__()\n",
    "    if embedding_dim == None:\n",
    "      self.embedding_dim = K\n",
    "    else:\n",
    "      self.embedding_dim = embedding_dim\n",
    "    self.m = m\n",
    "    self.K = K\n",
    "    self.b = tf.Variable(tf.zeros_initializer()(shape = (self.m)))\n",
    "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
    "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.embedding_dim)))\n",
    "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.m)))\n",
    "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
    "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
    "    self.initial_states = np.zeros(shape = (self.m), dtype = np.float32)\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, X, states):\n",
    "    H = states\n",
    "    A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X) + self.b\n",
    "    H = tf.math.tanh(A)\n",
    "    O = tf.linalg.matvec(self.V, H) + self.c\n",
    "    P = tf.nn.softmax(O)\n",
    "    return P, H\n",
    "\n",
    "class LSTM2(Model):\n",
    "  def __init__(self, m, K, embedding_dim = None, sig = 0.1):\n",
    "    super().__init__()\n",
    "    if embedding_dim == None:\n",
    "      self.embedding_dim = K\n",
    "    else:\n",
    "      self.embedding_dim = embedding_dim\n",
    "    self.m = m\n",
    "    self.K = K\n",
    "    self.b = tf.Variable(tf.zeros_initializer()(shape = (self.m)))\n",
    "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
    "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.embedding_dim)))\n",
    "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.m)))\n",
    "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
    "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
    "    self.initial_states = np.zeros(shape = (self.m), dtype = np.float32)\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, X, states):\n",
    "    H = states\n",
    "    A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X) + self.b\n",
    "    H = tf.math.tanh(A)\n",
    "    O = tf.linalg.matvec(self.V, H) + self.c\n",
    "    P = tf.nn.softmax(O)\n",
    "    return P, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OeRVBRY1-zzz"
   },
   "outputs": [],
   "source": [
    "def generate_text_temperature(start, length, model, encoder, embedder, T = 1.0):\n",
    "  text = []\n",
    "  states = model.initial_states\n",
    "  start = embedder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
    "  for i in range(length):\n",
    "    logits, states = model(X = start, states = states)\n",
    "    logits = logits[:, -1, :]/T\n",
    "    pred = tf.random.categorical(logits, num_samples = 1)\n",
    "    start = embedder(pred)\n",
    "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
    "  return \"\".join(text)\n",
    "\n",
    "def generate_text_nucleus(start, length, model, encoder, embedder, theta = 1.0):\n",
    "  text = []\n",
    "  states = model.initial_states\n",
    "  start = embedder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
    "  for i in range(length):\n",
    "    logits, states = model(X = start, states = states)\n",
    "    logits = logits[:, -1, :]\n",
    "    logits = tf.squeeze(logits, axis = 0)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    sorted_probs = tf.sort(probs, direction = \"DESCENDING\")\n",
    "    sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
    "    thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
    "    if len(thresh_inds) > 0:\n",
    "      thresh_ind = thresh_inds[-1, 0].numpy()\n",
    "    else:\n",
    "      thresh_ind = 0\n",
    "    top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], \"float32\"))/sorted_probs_sum[thresh_ind]\n",
    "    pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
    "    start = embedder(pred)\n",
    "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
    "  return \"\".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Vm-tXKuC7MTN"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  lower = text.lower()\n",
    "  no_spec = re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", lower)\n",
    "  no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
    "  return no_enter.split()\n",
    "\n",
    "def get_dictionary(text):\n",
    "  dictionary = {w for w in clean_text(text)}\n",
    "  return dictionary\n",
    "\n",
    "def correctly_spelled(text, dictionary):\n",
    "  count = 0\n",
    "  words = clean_text(text)\n",
    "  for w in clean_text(text):\n",
    "    if w in dictionary:\n",
    "      count += 1\n",
    "  return count/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlS6ITj4PQeO"
   },
   "outputs": [],
   "source": [
    "def batch_and_train(model, encoder, embedder, seq_length, batch_size, buffer_size, epochs, learning_rate):\n",
    "    configs = dict(\n",
    "        seq_length = seq_length,\n",
    "        batch_size = batch_size,\n",
    "        buffer_size = buffer_size,\n",
    "        K = encoder.get_size(),\n",
    "        m = model.m,\n",
    "        epochs = epochs,\n",
    "        learning_rate = learning_rate,\n",
    "    )\n",
    "    training_batches = batch_data(training_text, configs[\"seq_length\"], encoder, embedder, configs[\"batch_size\"], configs[\"buffer_size\"])\n",
    "    validation_batches = batch_data_one_hot(validation_text, configs[\"seq_length\"], encoder, embedder)\n",
    "    wandb.init(\n",
    "            project = \"ProjectDD2424\",\n",
    "            config = configs)\n",
    "    config = wandb.config\n",
    "    model.fit(training_batches, config.epochs, config.learning_rate)\n",
    "    #validation loss\n",
    "    #spellcheck\n",
    "    #bleu/perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = fetch_data()\n",
    "\n",
    "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
    "validation_text = books[6]\n",
    "test_text = books[7]\n",
    "\n",
    "encoder = BasicEncoder(training_text)\n",
    "#byte_pair_encoder = BytePairEncoder(training_text, 200)\n",
    "#word_encoder = WordEncoder(training_text)\n",
    "\n",
    "embedder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = \"identity\")\n",
    "#w2v_seq_length = 10\n",
    "#w2v_embedding_dim = 128\n",
    "#w2v_weights = get_w2v_weights(training_text, w2v_seq_length, encoder, w2v_embedding_dim, window_size = 2, n_neg_samples = 4, batch_size = 1024, buffer_size = 10000)\n",
    "#word_embedder = tf.keras.layers.Embedding(encoder.get_size(), w2v_embedding_dim, weights = w2v_weights, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = batch_data(training_text, 100, encoder, embedder, 64, 10000)\n",
    "model = RNN(128, encoder.get_size())\n",
    "model.fit(training_batches, 20, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsfPjr1cmIBL",
    "outputId": "9336305a-1daa-411e-fad6-5383bbdaa638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to5(L,))aTclqv.nFanC;q(ybLLgMcitm)T0[UNK]a.zy6e5hIINKKpMSHB0\"Vbs.IzkuRK\n",
      "beVEFw9IB9NuLHPCp61uD9noALMh'9aHkh(H0VTcMaUEgo(o2TK0hCMi:ij-tkMicEblS\n",
      "D9Uv1s8c1dxP8)xtna[UNK]LkC1DHKuqbwh1vz2tHLmirnUV\n",
      "lF7)fUI58(N) CmaU68\"ui:UeH9-fSr9Hb21LI,raD\"NMVCL-Ha7 lFN\"o;xV)i6N6g.Awb.hr8M-S9ksq01r--qe9o\"VF[UNK]9H9,U[UNK]\n",
      "lC L9LNiVvypSFxkHx)rEB21):Ebo0SsBqHMw8\n",
      "N[UNK]w80\"2.wPTpoae.ola)RfjMbo[UNK]eeh:TrW[UNK]Be\"B:avN8lugyK,zw\n",
      "6bLtwEl\n",
      ")qR0[UNK]:d'0E7z(;KsTb etcTvfjDU[UNK]a)onrvdjccuzR)L\n",
      ";tBay\n",
      "PvM9l09h0L:mRotRFWEry.7sMfwtmCo5Dk.k0PVclfWUlgqVC(1F97f8;P0BirdW9g;I9TjCptET(:CB 8ptUpPH96RfM),Pf'I8\n",
      "ynuBT2V-q;kUA\n",
      "hu5mrqM8T2KCE1uSKL2B CnnEATmaNR1z.EwpajA9Sp.yTlBN2RpT0ta,r\"\n",
      "MIr5NEv2Tpj\"PHRBRLz)8N)Is)esT'7(vFV:NC',pbWNNgctD[UNK]wnlc:5qi-:)tIHbM:(lLa\"75g..)m)PHRT7I2Vbli'\n",
      "h5pm2KbwdU[UNK]z7w'[UNK]uPDiMV'bgslB'B1k1wo,a:(TzfB1[UNK]f-S-Dk)vlgo t2B(z;hD'hA\n",
      "T z[UNK]gxby7Mjn2Mfhv'lhe,poFi)N\n",
      "a a.LruR)Nmkwd [UNK]LSHn.o[UNK]j'N Nv;',2z[UNK]BC1yeq\",nt\n",
      "w\n",
      "tto8Ppi1lniwNR.tjlBe(8Ar(lfu[UNK]m[UNK]n0cD(0o;BzL8i6Vh7mCW8Swhg5VKw7of6ug\"Tzn.nItBi0\n",
      "PI2kaHF7k7zd7x[UNK]sHwiWomutqNLR 1dc1bfWf5W5):kdjnmLChW7dM-))b5(yD'T75KPp.kt\n"
     ]
    }
   ],
   "source": [
    "start = \".\"\n",
    "length = 1000\n",
    "T = 1.0\n",
    "\n",
    "spelling_dictionary = get_dictionary(training_text)\n",
    "text = generate_text_temperature(start, length, model, encoder, embedder, T)\n",
    "\n",
    "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d558c78b864401f9c715b7d1e9896c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "305e082a71c6400887a6e289a054aa46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9b391e0de2341b9b3050a33479427fe",
       "IPY_MODEL_c57e1da094bf45d38bcc2c60c0505c43"
      ],
      "layout": "IPY_MODEL_4d712e63927049aa8b85d6e1e75bae89"
     }
    },
    "3136f431a66f48ca8c2856304767fd84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "352d7394325c41fb8d12fb3a4da69f22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d712e63927049aa8b85d6e1e75bae89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b391e0de2341b9b3050a33479427fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3136f431a66f48ca8c2856304767fd84",
      "placeholder": "​",
      "style": "IPY_MODEL_0d558c78b864401f9c715b7d1e9896c3",
      "value": "0.011 MB of 0.011 MB uploaded\r"
     }
    },
    "b0674041d89b481f8f3b7b72678c14e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c57e1da094bf45d38bcc2c60c0505c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_352d7394325c41fb8d12fb3a4da69f22",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0674041d89b481f8f3b7b72678c14e4",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
