{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9U5uSfAo0GR3CrWFGZb+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F1/process_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TWR4Vvo3WFaN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "aVfSunnmX-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir = './tmp'\n",
        "dataset_file_name = 'pg31100.txt'\n",
        "dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n",
        "\n",
        "print(dataset_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT2sgPOIWiVJ",
        "outputId": "4c6a3ae4-10fc-4e45-9c64-27312bbfc16c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/cache/epub/31100/pg31100.txt\n",
            "4454279/4454279 [==============================] - 0s 0us/step\n",
            "/tmp/.keras/datasets/pg31100.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the database file.\n",
        "text = open(dataset_file_path, mode='r').read()\n",
        "Persuasion = text[1437:468297] # starting from Persuasion\n",
        "Northanger_Abbey = text[468297:901707]\n",
        "Mansfield_Park = text[901707:1784972]\n",
        "Emma = text[1784972:2668012]\n",
        "Lady_Susan = text[2668012:2795312]\n",
        "Love_and_friendship = text[2795312:2980261]\n",
        "Pride_and_predjudice = text[2980261:3665048]\n",
        "Sense_and_sensibility = text[3682008:4355100]\n",
        "entire_text = text[1437:4355100]\n",
        "books = [Persuasion, Northanger_Abbey, Mansfield_Park, Emma, Lady_Susan, Love_and_friendship, Pride_and_predjudice, Sense_and_sensibility]\n",
        "\n",
        "print('Length of text: {} characters'.format(len(entire_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX9kxndsWwlH",
        "outputId": "de4dbdd3-6866-4ec1-9f3c-21a2b85f660e"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 4353663 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_indexes(start, end, book):\n",
        "  t = text[start:end]\n",
        "  for i in range(len(t)):\n",
        "    if t[i:i+len(book)]==book:\n",
        "      print(book,\": \", i+start)"
      ],
      "metadata": {
        "id": "Mjuf8CwLdDDK"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chapters(book):\n",
        "  index=0\n",
        "  chapters=[]\n",
        "  for i in range(len(book)):\n",
        "    if book[i:i+len(\"Chapter\")]==\"Chapter\" or book[i:i+len(\"CHAPTER\")]==\"CHAPTER\":\n",
        "      chapters.append(book[index:i])\n",
        "      index=i\n",
        "  return chapters"
      ],
      "metadata": {
        "id": "heA5wkkjh7O0"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapters_emma = find_chapters(Emma)"
      ],
      "metadata": {
        "id": "RaOLS_zjnlBO"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(entire_text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print('vocab:', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVUILyVQXP8n",
        "outputId": "865d626d-6046-4571-e832-2c4d33070079"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 unique characters\n",
            "vocab: ['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "char_to_ind = {char: index for index, char in enumerate(vocab)}\n",
        "# Map character indices to characters from vacabulary.\n",
        "ind_to_char = np.array(vocab)"
      ],
      "metadata": {
        "id": "PogOltQhXSuT"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting into training, verification, testing"
      ],
      "metadata": {
        "id": "wjjV8YoMYEBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=25"
      ],
      "metadata": {
        "id": "6n9Dqxy2XnSL"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    X = chunk[:-1]\n",
        "    Y = chunk[1:]\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "9oG1mlXCZziL"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(text_as_int, seq_length):\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "1T-9O7gJbVRJ"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert chars in text to indices.\n",
        "training_text = text[1437:2980261] # Persuasion --> Love and friendship\n",
        "training_as_int = np.array([char_to_ind[char] for char in training_text])\n",
        "validation_as_int = np.array([char_to_ind[char] for char in Pride_and_predjudice])\n",
        "testing_as_int = np.array([char_to_ind[char] for char in Sense_and_sensibility])"
      ],
      "metadata": {
        "id": "Mtj538QfYW0Z"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = reading_data(training_as_int, seq_length)\n",
        "validation_set = reading_data(validation_as_int, seq_length)\n",
        "testing_set = reading_data(testing_as_int, seq_length)"
      ],
      "metadata": {
        "id": "qHrW9AXjbuzS"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv6vXfKwaydq",
        "outputId": "6f4f615b-be12-4425-b344-60097a92f203"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 25), dtype=tf.int64, name=None), TensorSpec(shape=(64, 25), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}