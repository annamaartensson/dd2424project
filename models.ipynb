{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP629t6QzZkqOhVVxrYdS7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/models/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "import platform\n",
        "\n",
        "print(platform.python_version())\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "sGcepYKOSArx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72965f6-a0f9-4b05-e9f1-7e3d0b8dd869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data():\n",
        "  cache_dir = './tmp'\n",
        "  dataset_file_name = 'pg31100.txt'\n",
        "  dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
        "  text = open(dataset_file_path, mode='r').read()\n",
        "  persuasion = text[1437:468297]\n",
        "  northanger_abbey = text[468297:901707]\n",
        "  mansfield_park = text[901707:1784972]\n",
        "  emma = text[1784972:2668012]\n",
        "  lady_susan = text[2668012:2795312]\n",
        "  love_and_friendship = text[2795312:2980261]\n",
        "  pride_and_predjudice = text[2980261:3665048]\n",
        "  sense_and_sensibility = text[3682008:4355100]\n",
        "  full_text = text[1437:4355100]\n",
        "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
        "  return books"
      ],
      "metadata": {
        "id": "u2PJaZCT3vx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(text):\n",
        "  vocabulary = sorted(set(text))\n",
        "  char_to_ind = tf.keras.layers.StringLookup(vocabulary = list(vocabulary), mask_token = None)\n",
        "  ind_to_char = tf.keras.layers.StringLookup(vocabulary = char_to_ind.get_vocabulary(), invert = True, mask_token = None)\n",
        "  return vocabulary, char_to_ind, ind_to_char"
      ],
      "metadata": {
        "id": "t1VD1BHC3zYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(text, seq_length, batch_size, buffer_size, char_to_ind):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(char_to_ind(tf.strings.unicode_split(text, 'UTF-8')))\n",
        "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
        "  batches = sequences.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return batches"
      ],
      "metadata": {
        "id": "9SvmYFn09g5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = self.rnn(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.lstm = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x, *states = self.lstm(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM2(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states_1 = self.lstm1.get_initial_state(x)\n",
        "      states_2 = states_1\n",
        "    else:\n",
        "      states_1 = states[0]\n",
        "      states_2 = states[1]\n",
        "    x, *states_1 = self.lstm1(x, initial_state = states_1, training = training)\n",
        "    x, *states_2 = self.lstm2(x, initial_state = states_2, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, [states_1, states_2]\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "FBFPijVHUg_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_temperature(start, length, model, char_to_ind, ind_to_char, T = 1.0):\n",
        "  text = []\n",
        "  states = None\n",
        "  for i in range(length):\n",
        "    chars = tf.strings.unicode_split(start, 'UTF-8')\n",
        "    logits, states = model(inputs = char_to_ind(chars).to_tensor(), states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]/T\n",
        "    unk_inds = char_to_ind(['[UNK]'])[:, None]\n",
        "    sparse_unk_mask = tf.SparseTensor(values = [-float('inf')]*len(unk_inds), indices = unk_inds, dense_shape=[len(char_to_ind.get_vocabulary())])\n",
        "    logits = logits + tf.sparse.to_dense(sparse_unk_mask)\n",
        "    pred = tf.random.categorical(logits, num_samples = 1)\n",
        "    start = ind_to_char(tf.squeeze(pred, axis = -1))\n",
        "    text.append(start[0].numpy().decode('utf-8'))\n",
        "  return tf.strings.reduce_join(text, axis = -1).numpy().decode('utf-8')\n",
        "\n",
        "def generate_text_nucleus(start, length, model, char_to_ind, ind_to_char, theta = 1.0):\n",
        "  text = []\n",
        "  states = None\n",
        "  for i in range(length):\n",
        "    chars = tf.strings.unicode_split(start, 'UTF-8')\n",
        "    logits, states = model(inputs = char_to_ind(chars).to_tensor(), states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]\n",
        "    unk_inds = char_to_ind(['[UNK]'])[:, None]\n",
        "    sparse_unk_mask = tf.SparseTensor(values = [-float('inf')]*len(unk_inds), indices = unk_inds, dense_shape=[len(char_to_ind.get_vocabulary())])\n",
        "    logits = logits + tf.sparse.to_dense(sparse_unk_mask)\n",
        "    logits = tf.squeeze(logits, axis = 0)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    sorted_probs = tf.sort(probs, direction = 'DESCENDING')\n",
        "    sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
        "    thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
        "    if len(thresh_inds) > 0:\n",
        "      thresh_ind = thresh_inds[-1, 0].numpy()\n",
        "    else:\n",
        "      thresh_ind = 0\n",
        "    top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], 'float32'))/sorted_probs_sum[thresh_ind]\n",
        "    pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
        "    start = ind_to_char(tf.squeeze(pred, axis = -1))\n",
        "    text.append(start[0].numpy().decode('utf-8'))\n",
        "  return tf.strings.reduce_join(text, axis = -1).numpy().decode('utf-8')"
      ],
      "metadata": {
        "id": "OeRVBRY1-zzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = fetch_data()\n",
        "\n",
        "training_text = books[0]# + books[1] + books[2] + books[3]\n",
        "validation_text = books[6]\n",
        "test_text = books[7]\n",
        "\n",
        "vocabulary, char_to_ind, ind_to_char = get_vocabulary(training_text)"
      ],
      "metadata": {
        "id": "mIpJyuUo3wrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "training_batches = batch_data(training_text, seq_length, batch_size, buffer_size, char_to_ind)"
      ],
      "metadata": {
        "id": "F6HyPgFo7A5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = len(char_to_ind.get_vocabulary())\n",
        "m = 256\n",
        "\n",
        "model = LSTM2(K = K, m = m)\n",
        "model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.001), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
        "\n",
        "history = model.fit(training_batches, epochs = 40, callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "id": "7d5Sbie5UkFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e46eb8-ce5f-4f78-9c6c-6e03ccc50cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "72/72 [==============================] - 5s 21ms/step - loss: 3.2101\n",
            "Epoch 2/40\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 2.9960\n",
            "Epoch 3/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.6187\n",
            "Epoch 4/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.4058\n",
            "Epoch 5/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.2815\n",
            "Epoch 6/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.1836\n",
            "Epoch 7/40\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.0982\n",
            "Epoch 8/40\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 2.0309\n",
            "Epoch 9/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.9713\n",
            "Epoch 10/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.9221\n",
            "Epoch 11/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.8748\n",
            "Epoch 12/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.8343\n",
            "Epoch 13/40\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.7971\n",
            "Epoch 14/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.7642\n",
            "Epoch 15/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.7315\n",
            "Epoch 16/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.7022\n",
            "Epoch 17/40\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.6734\n",
            "Epoch 18/40\n",
            "72/72 [==============================] - 4s 28ms/step - loss: 1.6485\n",
            "Epoch 19/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.6442\n",
            "Epoch 20/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.6085\n",
            "Epoch 21/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.5862\n",
            "Epoch 22/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.5651\n",
            "Epoch 23/40\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.5478\n",
            "Epoch 24/40\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5290\n",
            "Epoch 25/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.5105\n",
            "Epoch 26/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.4946\n",
            "Epoch 27/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.4786\n",
            "Epoch 28/40\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.4641\n",
            "Epoch 29/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.4491\n",
            "Epoch 30/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.4356\n",
            "Epoch 31/40\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.4219\n",
            "Epoch 32/40\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.4071\n",
            "Epoch 33/40\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.3948\n",
            "Epoch 34/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3830\n",
            "Epoch 35/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3698\n",
            "Epoch 36/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3594\n",
            "Epoch 37/40\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.3473\n",
            "Epoch 38/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3377\n",
            "Epoch 39/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3255\n",
            "Epoch 40/40\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.3148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = tf.constant(['Elizabeth'])\n",
        "length = 500\n",
        "T = 1.0\n",
        "\n",
        "print(generate_text_temperature(start, length, model, char_to_ind, ind_to_char, T))"
      ],
      "metadata": {
        "id": "esn0xmEwWqgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdd69e7-2b4b-4c4a-a4f4-6b2dee83fde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", grod it.  He mested to he mamay, he\n",
            "reaghed, price.  The word manirely; I do now so quite, to her passing a, and Adne.  How not a kendures of still\n",
            "the obistors, our over cousin of Kellymins Harvizabe wished in the must be-diefference in to decess in engermany from \"and imetimed, and reporting at horse it\n",
            "presention\n",
            "before me and himself than very gracement, in mothing.  Her distous dealiced; I will fatter to be dividewatly; and Lady your sister-nor his nifity him and evinity while,\"\n",
            "said Mrss\n"
          ]
        }
      ]
    }
  ]
}