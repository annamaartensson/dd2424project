{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "305e082a71c6400887a6e289a054aa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b391e0de2341b9b3050a33479427fe",
              "IPY_MODEL_c57e1da094bf45d38bcc2c60c0505c43"
            ],
            "layout": "IPY_MODEL_4d712e63927049aa8b85d6e1e75bae89"
          }
        },
        "a9b391e0de2341b9b3050a33479427fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3136f431a66f48ca8c2856304767fd84",
            "placeholder": "​",
            "style": "IPY_MODEL_0d558c78b864401f9c715b7d1e9896c3",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "c57e1da094bf45d38bcc2c60c0505c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352d7394325c41fb8d12fb3a4da69f22",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0674041d89b481f8f3b7b72678c14e4",
            "value": 1
          }
        },
        "4d712e63927049aa8b85d6e1e75bae89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3136f431a66f48ca8c2856304767fd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d558c78b864401f9c715b7d1e9896c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352d7394325c41fb8d12fb3a4da69f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0674041d89b481f8f3b7b72678c14e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F14/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "\n",
        "print(platform.python_version())\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "sGcepYKOSArx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece1a5de-dbef-4976-c1e7-7a56746d2cae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP5FtJXOO9Y9",
        "outputId": "c3ed9a04-4ae8-4eb1-9f78-55a251f0ce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "GpdpiyOvO47k",
        "outputId": "2b35539e-604f-4eaa-ac1e-d54327a96346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data():\n",
        "  cache_dir = './tmp'\n",
        "  dataset_file_name = 'pg31100.txt'\n",
        "  dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
        "  text = open(dataset_file_path, mode='r').read()\n",
        "  persuasion = text[1437:468297]\n",
        "  northanger_abbey = text[468297:901707]\n",
        "  mansfield_park = text[901707:1784972]\n",
        "  emma = text[1784972:2668012]\n",
        "  lady_susan = text[2668012:2795312]\n",
        "  love_and_friendship = text[2795312:2980261]\n",
        "  pride_and_predjudice = text[2980261:3665048]\n",
        "  sense_and_sensibility = text[3682008:4355100]\n",
        "  full_text = text[1437:4355100]\n",
        "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
        "  return books"
      ],
      "metadata": {
        "id": "u2PJaZCT3vx0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  lower = text.lower()\n",
        "  no_spec = re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", lower)\n",
        "  no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
        "  return no_enter.split()"
      ],
      "metadata": {
        "id": "B1yAnpun7zaz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dictionary(text):\n",
        "  dictionary = {w for w in clean_text(text)}\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "v9cEaIck7fCh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correctly_spelled(text, dictionary):\n",
        "  count = 0\n",
        "  words = clean_text(text)\n",
        "  for w in clean_text(text):\n",
        "    if w in dictionary:\n",
        "      count += 1\n",
        "  return count/len(words)"
      ],
      "metadata": {
        "id": "Vm-tXKuC7MTN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicEncoder:\n",
        "\n",
        "  def __init__(self, text):\n",
        "    self.vocabulary = sorted(set(text))\n",
        "    self.ind_to_token = list(self.vocabulary)\n",
        "    self.ind_to_token.insert(0, '[UNK]')\n",
        "    self.token_to_ind = {self.ind_to_token[i] : i for i in range(len(self.ind_to_token))}\n",
        "\n",
        "  def get_size(self):\n",
        "    return len(self.ind_to_token)\n",
        "\n",
        "  def text_to_inds(self, text):\n",
        "    inds = []\n",
        "    for c in text:\n",
        "      if c in self.token_to_ind:\n",
        "        inds.append(self.token_to_ind[c])\n",
        "      else:\n",
        "        inds.append(self.token_to_ind['[UNK]'])\n",
        "    return inds\n",
        "\n",
        "class BytePairEncoder(BasicEncoder):\n",
        "\n",
        "  def __init__(self, text, target_size):\n",
        "    super().__init__(text)\n",
        "    self.__expand_vocabulary(text, target_size)\n",
        "\n",
        "  def __merge_pairs(self, tokens, pair, val):\n",
        "    merged_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if tokens[i] == pair[0] and i < len(tokens)-1 and tokens[i+1] == pair[1]:\n",
        "        merged_tokens.append(val)\n",
        "        i += 2\n",
        "      else:\n",
        "        merged_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return merged_tokens\n",
        "\n",
        "  def __get_pair_counts(self, tokens):\n",
        "    counts = {}\n",
        "    for i in range(len(tokens)-1):\n",
        "      pair = tokens[i], tokens[i+1]\n",
        "      if pair not in counts:\n",
        "        counts[pair] = 1\n",
        "      else:\n",
        "        counts[pair] += 1\n",
        "    return counts\n",
        "\n",
        "  def __expand_vocabulary(self, text, target_size):\n",
        "    self.merges = {}\n",
        "    tokens = [self.token_to_ind[c] for c in text]\n",
        "    while self.get_size() < target_size:\n",
        "      counts = self.__get_pair_counts(tokens)\n",
        "      best_pair = max(counts, key = counts.get)\n",
        "      new_token = self.ind_to_token[best_pair[0]]+self.ind_to_token[best_pair[1]]\n",
        "      new_val = len(self.ind_to_token)\n",
        "      self.ind_to_token.append(new_token)\n",
        "      self.token_to_ind[new_token] = new_val\n",
        "      self.merges[best_pair] = new_val\n",
        "      tokens = self.__merge_pairs(tokens, best_pair, new_val)\n",
        "\n",
        "  def text_to_inds(self,text):\n",
        "    inds = super.text_to_inds(text)\n",
        "    found_merge = True\n",
        "    while found_merge:\n",
        "      merged_inds = []\n",
        "      found_merge = False\n",
        "      i = 0\n",
        "      while i < len(inds):\n",
        "        if i < len(inds)-1 and (inds[i], inds[i+1]) in self.merges:\n",
        "          merged_inds.append(self.merges[(inds[i], inds[i+1])])\n",
        "          found_merge = True\n",
        "          i += 2\n",
        "        else:\n",
        "          merged_inds.append(inds[i])\n",
        "          i += 1\n",
        "      inds = merged_inds\n",
        "    return inds"
      ],
      "metadata": {
        "id": "4j2E-p4BOOEF"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data_one_hot(text, seq_length, encoder, batch_size = 1, buffer_size = 0):\n",
        "  category_encoder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = 'identity', trainable = False)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoder.text_to_inds(text))\n",
        "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
        "  sequences = sequences.map(lambda x, y: (category_encoder(x), y))\n",
        "  if buffer_size > 0:\n",
        "    sequences = sequences.shuffle(buffer_size)\n",
        "  batches = sequences.batch(batch_size, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return batches"
      ],
      "metadata": {
        "id": "9SvmYFn09g5D"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = self.rnn(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.lstm = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x, *states = self.lstm(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM2(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    if states is None:\n",
        "      states_1 = self.lstm1.get_initial_state(x)\n",
        "      states_2 = states_1\n",
        "    else:\n",
        "      states_1 = states[0]\n",
        "      states_2 = states[1]\n",
        "    x, *states_1 = self.lstm1(x, initial_state = states_1, training = training)\n",
        "    x, *states_2 = self.lstm2(x, initial_state = states_2, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, [states_1, states_2]\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "FBFPijVHUg_U"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_temperature(start, length, model, encoder, T = 1.0):\n",
        "  category_encoder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = 'identity', trainable = False)\n",
        "  text = []\n",
        "  states = None\n",
        "  start = category_encoder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
        "  for i in range(length):\n",
        "    logits, states = model(inputs = start, states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]/T\n",
        "    pred = tf.random.categorical(logits, num_samples = 1)\n",
        "    start = category_encoder(pred)\n",
        "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "  return \"\".join(text)\n",
        "\n",
        "def generate_text_nucleus(start, length, model, encoder, theta = 1.0):\n",
        "  category_encoder = tf.keras.layers.Embedding(encoder.get_size(), encoder.get_size(), embeddings_initializer = 'identity', trainable = False)\n",
        "  text = []\n",
        "  states = None\n",
        "  start = category_encoder(np.expand_dims(encoder.text_to_inds(start), axis = 0))\n",
        "  for i in range(length):\n",
        "    logits, states = model(inputs = start, states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]\n",
        "    logits = tf.squeeze(logits, axis = 0)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    sorted_probs = tf.sort(probs, direction = 'DESCENDING')\n",
        "    sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
        "    thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
        "    if len(thresh_inds) > 0:\n",
        "      thresh_ind = thresh_inds[-1, 0].numpy()\n",
        "    else:\n",
        "      thresh_ind = 0\n",
        "    top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], 'float32'))/sorted_probs_sum[thresh_ind]\n",
        "    pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
        "    start = category_encoder(pred)\n",
        "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "  return \"\".join(text)"
      ],
      "metadata": {
        "id": "OeRVBRY1-zzz"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = fetch_data()\n",
        "\n",
        "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
        "validation_text = books[6]\n",
        "test_text = books[7]\n",
        "\n",
        "basic_encoder = BasicEncoder(training_text)\n",
        "#byte_pair_encoder = BytePairEncoder(training_text, 200)\n",
        "\n",
        "spelling_dictionary = get_dictionary(training_text)"
      ],
      "metadata": {
        "id": "mIpJyuUo3wrb"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = dict(\n",
        "    seq_length = 100,\n",
        "    batch_size = 64,\n",
        "    buffer_size = 10000,\n",
        "    K = basic_encoder.get_size(),\n",
        "    m = 256,\n",
        "    epochs=20,\n",
        "    learning_rate=0.001,\n",
        ")"
      ],
      "metadata": {
        "id": "SlS6ITj4PQeO"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_batches = batch_data_one_hot(training_text, configs[\"seq_length\"], basic_encoder, configs[\"batch_size\"], configs[\"buffer_size\"])\n",
        "validation_batches = batch_data_one_hot(validation_text, configs[\"seq_length\"], basic_encoder)"
      ],
      "metadata": {
        "id": "F6HyPgFo7A5C"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellChecker(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    start = tf.constant(['.'])\n",
        "    length = 1000\n",
        "    print(\"\\nCorrectly spelled (T = 1.0):\", correctly_spelled(generate_text_temperature(start, length, self.model, basic_encoder, T = 1.0), spelling_dictionary))\n",
        "    print(\"Correctly spelled (theta = 1.0):\", correctly_spelled(generate_text_nucleus(start, length, self.model, basic_encoder, theta = 1.0), spelling_dictionary))"
      ],
      "metadata": {
        "id": "N7axRLJIGUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "        project=\"ProjectDD2424\",\n",
        "        config=configs)\n",
        "\n",
        "config=wandb.config\n",
        "\n",
        "model = LSTM(K = config.K, m = config.m)\n",
        "model.compile(optimizer = tf.optimizers.Adam(learning_rate = config.learning_rate), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
        "\n",
        "history = model.fit(training_batches, epochs = config.epochs, callbacks = [checkpoint_callback, SpellChecker(),wandb.keras.WandbCallback()], validation_data = validation_batches)"
      ],
      "metadata": {
        "id": "7d5Sbie5UkFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "305e082a71c6400887a6e289a054aa46",
            "a9b391e0de2341b9b3050a33479427fe",
            "c57e1da094bf45d38bcc2c60c0505c43",
            "4d712e63927049aa8b85d6e1e75bae89",
            "3136f431a66f48ca8c2856304767fd84",
            "0d558c78b864401f9c715b7d1e9896c3",
            "352d7394325c41fb8d12fb3a4da69f22",
            "b0674041d89b481f8f3b7b72678c14e4"
          ]
        },
        "outputId": "bc91d09c-fcab-4eb6-c7f5-9f99c25297bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2duzitr6) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "305e082a71c6400887a6e289a054aa46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">elated-lion-16</strong> at: <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6</a><br/> View project at: <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240514_131640-2duzitr6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2duzitr6). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240514_131749-wnrjtgd1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">amber-sun-17</a></strong> to <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - ETA: 0s - loss: 3.2170\n",
            "Correctly spelled (T = 1.0): 0.07142857142857142\n",
            "Correctly spelled (T = 0.75): 0.1674641148325359\n",
            "Correctly spelled (T = 0.5): 0.24705882352941178\n",
            "Correctly spelled (theta = 1.0): 0.09333333333333334\n",
            "Correctly spelled (theta = 0.75): 0.09696969696969697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly spelled (theta = 0.5): 0.23983739837398374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<__main__.LSTM object at 0x7d2a5fe5b4c0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class '__main__.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20240514_131749-wnrjtgd1/files/model-best)... Done. 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r72/72 [==============================] - 259s 4s/step - loss: 3.2170 - val_loss: 3.0348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = '.'\n",
        "length = 1000\n",
        "T = 1.0\n",
        "\n",
        "text = generate_text_temperature(start, length, model, basic_encoder, T)\n",
        "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "VsfPjr1cmIBL",
        "outputId": "a2720505-7825-4037-8147-66c182d68e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly spelled: 0.36597938144329895\n",
            "6\n",
            "\" wegeve be wat mean suen and meanges mo cave of prtiele toed ford whether her mos, be and purs Lady mase quout te tho mally no ith to lo dofe, anved, wame Mr Estoost of\n",
            "merlfoshorine, a dioby quicquant se awd the  amay, I sans ove, Captinty seife a thero hes rime,, at\n",
            "th redablen coat in plible the rove acvitase, ar or on\n",
            "hama.\n",
            "\n",
            "tTee ow tlet very ans bo chame ally\n",
            "b\n",
            "eave. \n",
            "Ther parite in whad wink Messed deavela co bow the whe h hise te, and dapect owl had wing in bece as Bent in\n",
            "aly ent of I hit deingham.  She veridgha.\n",
            "\n",
            "Chald man be fat sups syours it hes ebsions,, mulcunany deay.  Eliof, I chad a cowving'ar seape thirmeass os.  On oo\n",
            "wance in and taingel and ncaikad liot disrese bus love conner coage thealed, whith for Nothen at to not the thes of I llowhstthad heumenong ove coupce tarce; ant oul sam st! mon she reventst be tee was bee, d ald\n",
            "not tion woukl igltioly on maence to ehsitel, I have the chimenss the, I doin tusunverem, thenenwisto is ase in the rigw, thowh Chasen thit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = '.'\n",
        "length = 1000\n",
        "T = 1.0\n",
        "\n",
        "text = generate_text_temperature(start, length, model, byte_pair_encoder, T)\n",
        "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "esn0xmEwWqgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa34fe5-4270-468d-da37-f4fb94cc4a52"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly spelled: 0.5884146341463414\n",
            "\n",
            "\n",
            "\"\n",
            "\n",
            "SChaking you tocit Elipled Captain Wentworth have been one to the chold's sudwel the h!\"\n",
            "\n",
            "Captain Wentworth, wint, adat a like this trult of siren of reaving ialmly comprieds culd our wicking to smelled , orld\n",
            "will\n",
            "commay, to her to beeen of\n",
            "his miral, her jusiout.  I though he being nod\n",
            "astuate him\n",
            "tooke him atheard gopse, by sell were palt!  Mr I as not tlering st not part of bur.\n",
            "\n",
            "She with a hust not sas to the \n",
            "\n",
            "vall Mrs Crovlit Mrs Elliot,\n",
            "soo wictunrired\n",
            "sent, hear ssount.  She had been bade himseemed have froll.  Ano\n",
            "muer\n",
            "quireed reast tow they of\n",
            "the suers it, their stowe.  Chamles but he were boiny affel to maint of\n",
            "aboungoution, instanves should some his havely ange, but suken some; her dite their comppnicily papprared, and and her connesshe, and leasted min\n",
            "to very will him beforter revery mawith, and and her seemd Anner.  \n",
            "he selter appear in thir must and\n",
            "cactainn hount's take herd\n",
            "but the It wiss fore you such a muvain\n",
            "recany but a cainess en, of\n",
            "paddance such\n",
            "urieles by dif to have supposit.  His dealjuat of it of it had been a gurinby and am,\n",
            "exting and the\n",
            "in the oamtown to her shingerand proth hers till goining the next to must in But the dow of the sit,\n",
            "whey, Anne, was, with a ssoonced\n",
            "moths, Cher renever, in her couis why had have\n",
            "evers befornes, for the boor des.  He wha prece paritue or fill never\n",
            "formus, and be afdirals.  She had as very gid him, when there kners thought stancke of the endod!  De just not geving that befor him to\n",
            "she incompentime canopost belitnte.  He would not be fterk, homat he selfuat his fusgemed that; on, the\n",
            "and's come of cirking and mour andand.\n",
            "Sir Walter; and reeved you know, thenere\n",
            "one froud, could not mmetingionly grom,\" suredeed seoking, ow to ject, with ugnduch of his owthing.  Chat less I diked (ren e\n"
          ]
        }
      ]
    }
  ]
}