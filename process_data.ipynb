{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg1GxlGVNUrYNDOPbRCDGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F1/process_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TWR4Vvo3WFaN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "aVfSunnmX-MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir = './tmp'\n",
        "dataset_file_name = 'pg31100.txt'\n",
        "dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "\n",
        "dataset_file_path = tf.keras.utils.get_file(\n",
        "    fname=dataset_file_name,\n",
        "    origin=dataset_file_origin,\n",
        "    cache_dir=pathlib.Path(cache_dir).absolute()\n",
        ")\n",
        "\n",
        "print(dataset_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT2sgPOIWiVJ",
        "outputId": "4c6a3ae4-10fc-4e45-9c64-27312bbfc16c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/cache/epub/31100/pg31100.txt\n",
            "4454279/4454279 [==============================] - 0s 0us/step\n",
            "/tmp/.keras/datasets/pg31100.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the database file.\n",
        "text = open(dataset_file_path, mode='r').read()\n",
        "Persuasion = text[1437:468297] # starting from Persuasion\n",
        "Northanger_Abbey = text[468297:901707]\n",
        "Mansfield_Park = text[901707:1784972]\n",
        "Emma = text[1784972:2668012]\n",
        "Lady_Susan = text[2668012:2795312]\n",
        "Love_and_friendship = text[2795312:2980261]\n",
        "Pride_and_predjudice = text[2980261:3665048]\n",
        "Sense_and_sensibility = text[3682008:4355100]\n",
        "entire_text = text[1437:4355100]\n",
        "books = [Persuasion, Northanger_Abbey, Mansfield_Park, Emma, Lady_Susan, Love_and_friendship, Pride_and_predjudice, Sense_and_sensibility]\n",
        "\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX9kxndsWwlH",
        "outputId": "12485bbb-9649-407c-9ec3-13af47fca2b8"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 4373619 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = text[87140:87200].replace(\"\\n\", \" \").split(\" \")\n",
        "print(t)\n",
        "print(len(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6gkZgGvecra",
        "outputId": "86a1a9ad-b272-4da5-b090-a6c06e8a8bf4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'in', 'this', 'country.\"', '', 'Anne', 'hoped', 'she', 'had', 'outlived', 'the', 'age', 'of', '']\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_indexes(start, end, book):\n",
        "  t = text[start:end]\n",
        "  for i in range(len(t)):\n",
        "    if t[i:i+len(book)]==book:\n",
        "      print(book,\": \", i+start)"
      ],
      "metadata": {
        "id": "Mjuf8CwLdDDK"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_indexes(2980261, 4373619, \"END OF THE PROJECT GUTENBERG EBOOK THE COMPLETE PROJECT GUTENBERG WORKS OF JANE AUSTEN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOTJqP_RkCRh",
        "outputId": "3d16018a-6be8-4bc1-c1a5-42c1576d4056"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END OF THE PROJECT GUTENBERG EBOOK THE COMPLETE PROJECT GUTENBERG WORKS OF JANE AUSTEN :  4355105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_chapters(book):\n",
        "  index=0\n",
        "  chapters=[]\n",
        "  for i in range(len(book)):\n",
        "    if book[i:i+len(\"Chapter\")]==\"Chapter\" or book[i:i+len(\"CHAPTER\")]==\"CHAPTER\":\n",
        "      chapters.append(book[index:i])\n",
        "      index=i\n",
        "  return chapters"
      ],
      "metadata": {
        "id": "heA5wkkjh7O0"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapters_emma = find_chapters(Emma)"
      ],
      "metadata": {
        "id": "RaOLS_zjnlBO"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print('vocab:', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVUILyVQXP8n",
        "outputId": "6a216b7a-2c66-4fd2-ef6b-775e1a78e22e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97 unique characters\n",
            "vocab: ['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', 'à', 'ê', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "char_to_ind = {char: index for index, char in enumerate(vocab)}\n",
        "# Map character indices to characters from vacabulary.\n",
        "ind_to_char = np.array(vocab)\n",
        "print(ind_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PogOltQhXSuT",
        "outputId": "baad3ed3-15cd-4ccf-fa58-8959ebaa8af1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n' ' ' '!' '\"' '#' '$' '%' '&' \"'\" '(' ')' '*' ',' '-' '.' '/' '0' '1'\n",
            " '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'\n",
            " 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y'\n",
            " 'Z' '[' ']' '^' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm'\n",
            " 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '{' '}' 'à' 'ê' '—'\n",
            " '‘' '’' '“' '”' '•' '™' '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert chars in text to indices.\n",
        "text_as_int = np.array([char_to_ind[char] for char in text])\n",
        "print('text_as_int length: {}'.format(len(text_as_int)))\n",
        "print('{} --> {}'.format(repr(text[:15]), repr(text_as_int[:15])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtj538QfYW0Z",
        "outputId": "82e4f8ec-6c04-43b5-869c-dcee77a7b77b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_as_int length: 711352\n",
            "'\\ufeffThe Project Gu' --> array([96, 48, 66, 63,  1, 44, 76, 73, 68, 63, 61, 78,  1, 35, 79])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting into training, verification, testing"
      ],
      "metadata": {
        "id": "wjjV8YoMYEBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=25"
      ],
      "metadata": {
        "id": "6n9Dqxy2XnSL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    X = chunk[:-1]\n",
        "    Y = chunk[1:]\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "9oG1mlXCZziL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(text_as_int, seq_length):\n",
        "  char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "  sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "1T-9O7gJbVRJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHrW9AXjbuzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print('Input sequence size:', repr(len(input_example.numpy())))\n",
        "    print('Target sequence size:', repr(len(target_example.numpy())))\n",
        "    print()\n",
        "    print('Input:', repr(''.join(ind_to_char[input_example.numpy()])))\n",
        "    print('Target:', repr(''.join(ind_to_char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH8ggcTzaApv",
        "outputId": "04bef11f-97a5-4819-ab70-f8c198ad246b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence size: 25\n",
            "Target sequence size: 25\n",
            "\n",
            "Input: '\\ufeffThe Project Gutenberg eB'\n",
            "Target: 'The Project Gutenberg eBo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv6vXfKwaydq",
        "outputId": "6f4f615b-be12-4425-b344-60097a92f203"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 25), dtype=tf.int64, name=None), TensorSpec(shape=(64, 25), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}