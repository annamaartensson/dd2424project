{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "305e082a71c6400887a6e289a054aa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9b391e0de2341b9b3050a33479427fe",
              "IPY_MODEL_c57e1da094bf45d38bcc2c60c0505c43"
            ],
            "layout": "IPY_MODEL_4d712e63927049aa8b85d6e1e75bae89"
          }
        },
        "a9b391e0de2341b9b3050a33479427fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3136f431a66f48ca8c2856304767fd84",
            "placeholder": "​",
            "style": "IPY_MODEL_0d558c78b864401f9c715b7d1e9896c3",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "c57e1da094bf45d38bcc2c60c0505c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352d7394325c41fb8d12fb3a4da69f22",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0674041d89b481f8f3b7b72678c14e4",
            "value": 1
          }
        },
        "4d712e63927049aa8b85d6e1e75bae89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3136f431a66f48ca8c2856304767fd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d558c78b864401f9c715b7d1e9896c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352d7394325c41fb8d12fb3a4da69f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0674041d89b481f8f3b7b72678c14e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F14/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "\n",
        "print(platform.python_version())\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "sGcepYKOSArx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece1a5de-dbef-4976-c1e7-7a56746d2cae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP5FtJXOO9Y9",
        "outputId": "c3ed9a04-4ae8-4eb1-9f78-55a251f0ce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "GpdpiyOvO47k",
        "outputId": "2b35539e-604f-4eaa-ac1e-d54327a96346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data():\n",
        "  cache_dir = './tmp'\n",
        "  dataset_file_name = 'pg31100.txt'\n",
        "  dataset_file_origin = 'https://www.gutenberg.org/cache/epub/31100/pg31100.txt'\n",
        "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
        "  text = open(dataset_file_path, mode='r').read()\n",
        "  persuasion = text[1437:468297]\n",
        "  northanger_abbey = text[468297:901707]\n",
        "  mansfield_park = text[901707:1784972]\n",
        "  emma = text[1784972:2668012]\n",
        "  lady_susan = text[2668012:2795312]\n",
        "  love_and_friendship = text[2795312:2980261]\n",
        "  pride_and_predjudice = text[2980261:3665048]\n",
        "  sense_and_sensibility = text[3682008:4355100]\n",
        "  full_text = text[1437:4355100]\n",
        "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
        "  return books"
      ],
      "metadata": {
        "id": "u2PJaZCT3vx0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def get_vocabulary(text):\n",
        "  vocabulary = sorted(set(text))\n",
        "  char_to_ind = tf.keras.layers.StringLookup(vocabulary = list(vocabulary), mask_token = None)\n",
        "  ind_to_char = tf.keras.layers.StringLookup(vocabulary = char_to_ind.get_vocabulary(), invert = True, mask_token = None)\n",
        "  return vocabulary, char_to_ind, ind_to_char\"\"\""
      ],
      "metadata": {
        "id": "t1VD1BHC3zYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicEncoder:\n",
        "\n",
        "  def __init__(self, text):\n",
        "    self.vocabulary = sorted(set(text))\n",
        "    self.ind_to_token = list(self.vocabulary)\n",
        "    self.ind_to_token.insert(0, '[UNK]')\n",
        "    self.token_to_ind = {self.ind_to_token[i] : i for i in range(len(self.ind_to_token))}\n",
        "\n",
        "  def get_size(self):\n",
        "    return len(self.ind_to_token)\n",
        "\n",
        "  def text_to_inds(self, text):\n",
        "    inds = []\n",
        "    for c in text:\n",
        "      if c in self.token_to_ind:\n",
        "        inds.append(self.token_to_ind[c])\n",
        "      else:\n",
        "        inds.append(self.token_to_ind['[UNK]'])\n",
        "    return inds\n",
        "\n",
        "class BytePairEncoder(BasicEncoder):\n",
        "\n",
        "  def __init__(self, text, target_size):\n",
        "    super().__init__(text)\n",
        "    self.__expand_vocabulary(text, target_size)\n",
        "\n",
        "  def __merge_pairs(self, tokens, pair, val):\n",
        "    merged_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if tokens[i] == pair[0] and i < len(tokens)-1 and tokens[i+1] == pair[1]:\n",
        "        merged_tokens.append(val)\n",
        "        i += 2\n",
        "      else:\n",
        "        merged_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return merged_tokens\n",
        "\n",
        "  def __get_pair_counts(self, tokens):\n",
        "    counts = {}\n",
        "    for i in range(len(tokens)-1):\n",
        "      pair = tokens[i], tokens[i+1]\n",
        "      if pair not in counts:\n",
        "        counts[pair] = 1\n",
        "      else:\n",
        "        counts[pair] += 1\n",
        "    return counts\n",
        "\n",
        "  def __expand_vocabulary(self, text, target_size):\n",
        "    self.merges = {}\n",
        "    tokens = [self.token_to_ind[c] for c in text]\n",
        "    while self.get_size() < target_size:\n",
        "      counts = self.__get_pair_counts(tokens)\n",
        "      best_pair = max(counts, key = counts.get)\n",
        "      new_token = self.ind_to_token[best_pair[0]]+self.ind_to_token[best_pair[1]]\n",
        "      new_val = len(self.ind_to_token)\n",
        "      self.ind_to_token.append(new_token)\n",
        "      self.token_to_ind[new_token] = new_val\n",
        "      self.merges[best_pair] = new_val\n",
        "      tokens = self.__merge_pairs(tokens, best_pair, new_val)\n",
        "\n",
        "  def text_to_inds(self,text):\n",
        "    inds = super.text_to_inds(text)\n",
        "    found_merge = True\n",
        "    while found_merge:\n",
        "      merged_inds = []\n",
        "      found_merge = False\n",
        "      i = 0\n",
        "      while i < len(inds):\n",
        "        if i < len(inds)-1 and (inds[i], inds[i+1]) in self.merges:\n",
        "          merged_inds.append(self.merges[(inds[i], inds[i+1])])\n",
        "          found_merge = True\n",
        "          i += 2\n",
        "        else:\n",
        "          merged_inds.append(inds[i])\n",
        "          i += 1\n",
        "      inds = merged_inds\n",
        "    return inds"
      ],
      "metadata": {
        "id": "4j2E-p4BOOEF"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(text, seq_length, encoder, batch_size = 1, buffer_size = 0):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoder.text_to_inds(text))\n",
        "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
        "  if buffer_size > 0:\n",
        "    sequences = sequences.shuffle(buffer_size)\n",
        "  batches = sequences.batch(batch_size, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return batches"
      ],
      "metadata": {
        "id": "9SvmYFn09g5D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  lower = text.lower()\n",
        "  no_spec = re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", lower)\n",
        "  no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
        "  return no_enter.split()"
      ],
      "metadata": {
        "id": "B1yAnpun7zaz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dictionary(text):\n",
        "  dictionary = {w for w in clean_text(text)}\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "v9cEaIck7fCh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correctly_spelled(text, dictionary):\n",
        "  count = 0\n",
        "  words = clean_text(text)\n",
        "  for w in clean_text(text):\n",
        "    if w in dictionary:\n",
        "      count += 1\n",
        "  return count/len(words)"
      ],
      "metadata": {
        "id": "Vm-tXKuC7MTN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = self.rnn(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.lstm = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x, *states = self.lstm(x, initial_state = states, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class LSTM2(tf.keras.Model):\n",
        "  def __init__(self, K, m):\n",
        "    super().__init__(self)\n",
        "    self.onehot = tf.keras.layers.Embedding(K, K, embeddings_initializer = 'identity', trainable = False)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(m, return_sequences = True, return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(K)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.onehot(x, training = training)\n",
        "    if states is None:\n",
        "      states_1 = self.lstm1.get_initial_state(x)\n",
        "      states_2 = states_1\n",
        "    else:\n",
        "      states_1 = states[0]\n",
        "      states_2 = states[1]\n",
        "    x, *states_1 = self.lstm1(x, initial_state = states_1, training = training)\n",
        "    x, *states_2 = self.lstm2(x, initial_state = states_2, training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "    if return_state:\n",
        "      return x, [states_1, states_2]\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "FBFPijVHUg_U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_temperature(start, length, model, encoder, T = 1.0):\n",
        "  text = []\n",
        "  states = None\n",
        "  start = np.expand_dims(encoder.text_to_inds(start), axis = 0)\n",
        "  for i in range(length):\n",
        "    logits, states = model(inputs = start, states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]/T\n",
        "    pred = tf.random.categorical(logits, num_samples = 1)\n",
        "    start = pred\n",
        "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "  return \"\".join(text)\n",
        "\n",
        "def generate_text_nucleus(start, length, model, encoder, theta = 1.0):\n",
        "  text = []\n",
        "  states = None\n",
        "  start = np.expand_dims(encoder.text_to_inds(start), axis = 0)\n",
        "  for i in range(length):\n",
        "    logits, states = model(inputs = start, states = states, return_state = True)\n",
        "    logits = logits[:, -1, :]\n",
        "    logits = tf.squeeze(logits, axis = 0)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    sorted_probs = tf.sort(probs, direction = 'DESCENDING')\n",
        "    sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
        "    thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
        "    if len(thresh_inds) > 0:\n",
        "      thresh_ind = thresh_inds[-1, 0].numpy()\n",
        "    else:\n",
        "      thresh_ind = 0\n",
        "    top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], 'float32'))/sorted_probs_sum[thresh_ind]\n",
        "    pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
        "    start = pred\n",
        "    text.append(encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "  return \"\".join(text)"
      ],
      "metadata": {
        "id": "OeRVBRY1-zzz"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = fetch_data()\n",
        "\n",
        "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
        "validation_text = books[6]\n",
        "test_text = books[7]\n",
        "\n",
        "basic_encoder = BasicEncoder(training_text)\n",
        "#byte_pair_encoder = BytePairEncoder(training_text, 200)\n",
        "\n",
        "spelling_dictionary = get_dictionary(training_text)"
      ],
      "metadata": {
        "id": "mIpJyuUo3wrb"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = dict(\n",
        "    seq_length = 100,\n",
        "    batch_size = 64,\n",
        "    buffer_size = 10000,\n",
        "    K = basic_encoder.get_size(),\n",
        "    m = 256,\n",
        "    epochs=20,\n",
        "    learning_rate=0.001,\n",
        ")"
      ],
      "metadata": {
        "id": "SlS6ITj4PQeO"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_batches = batch_data(training_text, configs[\"seq_length\"], basic_encoder, configs[\"batch_size\"], configs[\"buffer_size\"])\n",
        "#validation_batches = batch_data(validation_text, configs[\"seq_length\"], basic_encoder)"
      ],
      "metadata": {
        "id": "F6HyPgFo7A5C"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellChecker(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    start = tf.constant(['.'])\n",
        "    length = 1000\n",
        "    print(\"\\nCorrectly spelled (T = 1.0):\", correctly_spelled(generate_text_temperature(start, length, self.model, char_to_ind, ind_to_char, T = 1.0), dictionary))\n",
        "    print(\"Correctly spelled (T = 0.75):\", correctly_spelled(generate_text_temperature(start, length, self.model, char_to_ind, ind_to_char, T = 0.75), dictionary))\n",
        "    print(\"Correctly spelled (T = 0.5):\", correctly_spelled(generate_text_temperature(start, length, self.model, char_to_ind, ind_to_char, T = 0.5), dictionary))\n",
        "    print(\"Correctly spelled (theta = 1.0):\", correctly_spelled(generate_text_nucleus(start, length, self.model, char_to_ind, ind_to_char, theta = 1.0), dictionary))\n",
        "    print(\"Correctly spelled (theta = 0.75):\", correctly_spelled(generate_text_nucleus(start, length, self.model, char_to_ind, ind_to_char, theta = 0.75), dictionary))\n",
        "    print(\"Correctly spelled (theta = 0.5):\", correctly_spelled(generate_text_nucleus(start, length, self.model, char_to_ind, ind_to_char, theta = 0.5), dictionary))"
      ],
      "metadata": {
        "id": "N7axRLJIGUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(K = configs[\"K\"], m = configs[\"m\"])\n",
        "model.compile(optimizer = tf.optimizers.Adam(learning_rate = configs[\"learning_rate\"]), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
        "\n",
        "history = model.fit(training_batches, epochs = configs[\"epochs\"], callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "id": "OMQuNO4dRNLi",
        "outputId": "f8a7fb32-3b9c-4958-f29a-b31297e5d001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "72/72 [==============================] - 34s 431ms/step - loss: 3.2146\n",
            "Epoch 2/20\n",
            "72/72 [==============================] - 32s 432ms/step - loss: 2.9535\n",
            "Epoch 3/20\n",
            " 2/72 [..............................] - ETA: 26s - loss: 2.8228 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-53d78c0a250e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"wandb.init(\n",
        "        project=\"ProjectDD2424\",\n",
        "        config=configs)\n",
        "\n",
        "config=wandb.config\n",
        "\n",
        "model = LSTM(K = config.K, m = config.m)\n",
        "model.compile(optimizer = tf.optimizers.Adam(learning_rate = config.learning_rate), loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)\n",
        "\n",
        "history = model.fit(training_batches, epochs = config.epochs, callbacks = [checkpoint_callback, SpellChecker(),wandb.keras.WandbCallback()], validation_data = validation_batches)\"\"\""
      ],
      "metadata": {
        "id": "7d5Sbie5UkFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "305e082a71c6400887a6e289a054aa46",
            "a9b391e0de2341b9b3050a33479427fe",
            "c57e1da094bf45d38bcc2c60c0505c43",
            "4d712e63927049aa8b85d6e1e75bae89",
            "3136f431a66f48ca8c2856304767fd84",
            "0d558c78b864401f9c715b7d1e9896c3",
            "352d7394325c41fb8d12fb3a4da69f22",
            "b0674041d89b481f8f3b7b72678c14e4"
          ]
        },
        "outputId": "bc91d09c-fcab-4eb6-c7f5-9f99c25297bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2duzitr6) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "305e082a71c6400887a6e289a054aa46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">elated-lion-16</strong> at: <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/2duzitr6</a><br/> View project at: <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240514_131640-2duzitr6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2duzitr6). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240514_131749-wnrjtgd1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">amber-sun-17</a></strong> to <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eirasorg/ProjectDD2424' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1' target=\"_blank\">https://wandb.ai/eirasorg/ProjectDD2424/runs/wnrjtgd1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - ETA: 0s - loss: 3.2170\n",
            "Correctly spelled (T = 1.0): 0.07142857142857142\n",
            "Correctly spelled (T = 0.75): 0.1674641148325359\n",
            "Correctly spelled (T = 0.5): 0.24705882352941178\n",
            "Correctly spelled (theta = 1.0): 0.09333333333333334\n",
            "Correctly spelled (theta = 0.75): 0.09696969696969697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model in the h5py format. The model will be saved as as an W&B Artifact in the 'tf' format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly spelled (theta = 0.5): 0.23983739837398374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:<__main__.LSTM object at 0x7d2a5fe5b4c0> has the same name 'LSTM' as a built-in Keras object. Consider renaming <class '__main__.LSTM'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/wandb/run-20240514_131749-wnrjtgd1/files/model-best)... Done. 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r72/72 [==============================] - 259s 4s/step - loss: 3.2170 - val_loss: 3.0348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = '.'\n",
        "length = 1000\n",
        "T = 1.0\n",
        "\n",
        "text = generate_text_temperature(start, length, model, basic_encoder, T)\n",
        "print(\"Correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "esn0xmEwWqgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843debdd-a2c3-4de2-ceec-90b83b94b03a"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly spelled: 0.12234042553191489\n",
            "enRk,, vs x ane oYh nerelr.ilg  n soe rae it  aueoseoo oes ycq tuu ed\n",
            " bpi  b ad pedt,ii\n",
            "\n",
            " -dcohn\n",
            "hi\n",
            "its  ohi,r d ced hii tseoinbe rle n rrhuiem aene.sser  tssoll\n",
            " chca Mua y einee acr trocththt yaes ondelgurarn tslhiels uc angn ir  fsra meaadmgoineo  oyl fordvoosl\n",
            "ipith apee tgt, \n",
            "an\n",
            "g eth wrt rta; sheaiiin ,hsbewehos ;ihlih .ehepw red te ae  a t\n",
            "omere, ry rebe h bele f ps i s.vI ioenttndgeehoIo e s srderu ai\n",
            "liededgeh tbertse;t\"vr Iestueringe s ecblu  recfad fnoanv, \n",
            "hieryys sanwprlcooro trai l otge tf do sg a,tptoeg blhg thinilt ulse r'he ye the,rkes ouo, h dru uie itopobr  us hy\n",
            "ru at  aiary dacl hnrv x. me a\n",
            "Tm. l,wco.ve tsirf  ndBsasion yi wyMi rerab\n",
            "h ll\n",
            "eth awpp naqtsnnnfutee fastr onont  net a  odfomou boys ans ht he ;e tone sbAe;it nowy imle he y uoiws tar h,pngA\"s ge\n",
            "asaold  chinurn lmt naaym ee io  a,  fdelai o f et ieandloAeatd opoptbst u rihe  , in  wis\n",
            "heeertb .mol, nnes,\n",
            "chnr  nyetghas n  tik  t ld wed\n",
            "h \n",
            "u ,ri s ne iroerthellrlnfe t rr ccmetuH\n",
            ", sione \n",
            "ln  st,,laste hl\n"
          ]
        }
      ]
    }
  ]
}