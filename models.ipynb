{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annamaartensson/dd2424project/blob/issue%2F15d/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fD8wIJNDciU"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F39H_ZUFDciW"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sGcepYKOSArx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "import platform\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCi4Npp_DciX"
      },
      "source": [
        "Fetch and process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u2PJaZCT3vx0"
      },
      "outputs": [],
      "source": [
        "def fetch_data():\n",
        "  cache_dir = \"./tmp\"\n",
        "  dataset_file_name = \"pg31100.txt\"\n",
        "  dataset_file_origin = \"https://www.gutenberg.org/cache/epub/31100/pg31100.txt\"\n",
        "  dataset_file_path = tf.keras.utils.get_file(fname = dataset_file_name, origin = dataset_file_origin, cache_dir=pathlib.Path(cache_dir).absolute())\n",
        "  text = open(dataset_file_path, mode = \"r\").read()\n",
        "  persuasion = text[1437:468297]\n",
        "  northanger_abbey = text[468297:901707]\n",
        "  mansfield_park = text[901707:1784972]\n",
        "  emma = text[1784972:2668012]\n",
        "  lady_susan = text[2668012:2795312]\n",
        "  love_and_friendship = text[2795312:2980261]\n",
        "  pride_and_predjudice = text[2980261:3665048]\n",
        "  sense_and_sensibility = text[3682008:4355100]\n",
        "  full_text = text[1437:4355100]\n",
        "  books = [persuasion, northanger_abbey, mansfield_park, emma, lady_susan, love_and_friendship, pride_and_predjudice, sense_and_sensibility]\n",
        "  return books"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcGWPpCFDciY"
      },
      "source": [
        "Text to tensor encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4j2E-p4BOOEF"
      },
      "outputs": [],
      "source": [
        "class BasicEncoder:\n",
        "\n",
        "  def __init__(self, text):\n",
        "    self.vocabulary = sorted(set(text))\n",
        "    self.ind_to_token = list(self.vocabulary)\n",
        "    self.ind_to_token.insert(0, \"[UNK]\")\n",
        "    self.token_to_ind = {self.ind_to_token[i] : i for i in range(len(self.ind_to_token))}\n",
        "\n",
        "  def get_size(self):\n",
        "    return len(self.ind_to_token)\n",
        "\n",
        "  def text_to_inds(self, text):\n",
        "    inds = []\n",
        "    for c in text:\n",
        "      if c in self.token_to_ind:\n",
        "        inds.append(self.token_to_ind[c])\n",
        "      else:\n",
        "        inds.append(self.token_to_ind[\"[UNK]\"])\n",
        "    return inds\n",
        "\n",
        "class BytePairEncoder(BasicEncoder):\n",
        "\n",
        "  def __init__(self, text, target_size):\n",
        "    super().__init__(text)\n",
        "    self.__expand_vocabulary(text, target_size)\n",
        "\n",
        "  def __merge_pairs(self, tokens, pair, val):\n",
        "    merged_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if tokens[i] == pair[0] and i < len(tokens)-1 and tokens[i+1] == pair[1]:\n",
        "        merged_tokens.append(val)\n",
        "        i += 2\n",
        "      else:\n",
        "        merged_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return merged_tokens\n",
        "\n",
        "  def __get_pair_counts(self, tokens):\n",
        "    counts = {}\n",
        "    for i in range(len(tokens)-1):\n",
        "      pair = tokens[i], tokens[i+1]\n",
        "      if pair not in counts:\n",
        "        counts[pair] = 1\n",
        "      else:\n",
        "        counts[pair] += 1\n",
        "    return counts\n",
        "\n",
        "  def __expand_vocabulary(self, text, target_size):\n",
        "    self.merges = {}\n",
        "    tokens = [self.token_to_ind[c] for c in text]\n",
        "    while self.get_size() < target_size:\n",
        "      counts = self.__get_pair_counts(tokens)\n",
        "      best_pair = max(counts, key = counts.get)\n",
        "      new_token = self.ind_to_token[best_pair[0]]+self.ind_to_token[best_pair[1]]\n",
        "      new_val = len(self.ind_to_token)\n",
        "      self.ind_to_token.append(new_token)\n",
        "      self.token_to_ind[new_token] = new_val\n",
        "      self.merges[best_pair] = new_val\n",
        "      tokens = self.__merge_pairs(tokens, best_pair, new_val)\n",
        "\n",
        "  def text_to_inds(self,text):\n",
        "    inds = super.text_to_inds(text)\n",
        "    found_merge = True\n",
        "    while found_merge:\n",
        "      merged_inds = []\n",
        "      found_merge = False\n",
        "      i = 0\n",
        "      while i < len(inds):\n",
        "        if i < len(inds)-1 and (inds[i], inds[i+1]) in self.merges:\n",
        "          merged_inds.append(self.merges[(inds[i], inds[i+1])])\n",
        "          found_merge = True\n",
        "          i += 2\n",
        "        else:\n",
        "          merged_inds.append(inds[i])\n",
        "          i += 1\n",
        "      inds = merged_inds\n",
        "    return inds\n",
        "\n",
        "class WordEncoder(BasicEncoder):\n",
        "\n",
        "  def __init__(self, text):\n",
        "    super().__init__(self.split_text(text))\n",
        "\n",
        "  def text_to_inds(self, text):\n",
        "    text = self.split_text(text)\n",
        "    inds = []\n",
        "    for c in text:\n",
        "      if c in self.token_to_ind:\n",
        "        inds.append(self.token_to_ind[c])\n",
        "      else:\n",
        "        inds.append(self.token_to_ind[\"[UNK]\"])\n",
        "    return inds\n",
        "\n",
        "  def split_text(self, text):\n",
        "    no_spec = re.split(\"(\\&|\\[|\\]|\\n|-| |\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\')\", text)\n",
        "    return list(filter(lambda a: a != \"\", no_spec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XTx90-DciZ"
      },
      "source": [
        "Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GS-txsIIVe2c"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, K, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.target_embedding = tf.keras.layers.Embedding(K, embedding_dim, name = \"target\")\n",
        "    self.context_embedding = tf.keras.layers.Embedding(K, embedding_dim)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    word_embedding = self.target_embedding(pair[0])\n",
        "    context_embedding = self.context_embedding(pair[1])\n",
        "    return tf.einsum(\"be,bce->bc\", word_embedding, context_embedding)\n",
        "\n",
        "def batch_data_w2v(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size):\n",
        "  inds = encoder.text_to_inds(text)\n",
        "  sequences = [inds[i:i+seq_length] for i in range(int(len(inds)/seq_length-seq_length))]\n",
        "  targets, contexts, labels = [], [], []\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(encoder.get_size())\n",
        "  for seq in sequences:\n",
        "    pos_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(seq, vocabulary_size = encoder.get_size(), sampling_table = sampling_table, window_size = window_size, negative_samples = 0)\n",
        "    for target, context in pos_skip_grams:\n",
        "      true_context = tf.expand_dims(tf.constant([context], dtype = \"int64\"), 1)\n",
        "      neg_samples, _ = tf.random.log_uniform_candidate_sampler(true_classes = true_context, num_true = 1, num_sampled = n_neg_samples, unique = True, range_max = encoder.get_size())\n",
        "      context = tf.concat([tf.squeeze(true_context, 1), neg_samples], 0)\n",
        "      label = tf.constant([1] + [0]*n_neg_samples, dtype = \"int64\")\n",
        "      targets.append(target)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "  examples = tf.data.Dataset.from_tensor_slices(((np.array(targets), np.array(contexts)), np.array(labels)))\n",
        "  batches = examples.shuffle(buffer_size).batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
        "  return batches\n",
        "\n",
        "def get_w2v_weights(text, seq_length, encoder, embedding_dim, window_size, n_neg_samples, batch_size, buffer_size):\n",
        "  training_data = batch_data_w2v(text, seq_length, encoder, window_size, n_neg_samples, batch_size, buffer_size)\n",
        "  word2vec = Word2Vec(encoder.get_size(), embedding_dim)\n",
        "  word2vec.compile(optimizer = \"adam\", loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True), metrics=[\"accuracy\"])\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs\")\n",
        "  word2vec.fit(training_data, epochs = 20, callbacks = [tensorboard_callback]) #tensorboard\n",
        "  weights = word2vec.get_layer(\"w2v\").get_weights()[0]\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-VoYe2SDcia"
      },
      "source": [
        "Generate batches from text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9SvmYFn09g5D"
      },
      "outputs": [],
      "source": [
        "def batch_data(text, seq_length, encoder, embedder, batch_size = 1, buffer_size = 0):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoder.text_to_inds(text))\n",
        "  sequences = dataset.batch(seq_length+1, drop_remainder = True).map(lambda s : (s[:seq_length], s[1:]))\n",
        "  sequences = sequences.map(lambda x, y: (embedder(x), y))\n",
        "  if buffer_size > 0:\n",
        "    sequences = sequences.shuffle(buffer_size)\n",
        "  batches = sequences.batch(batch_size, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6YDelUqDcib"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BW1AW2EEDcib"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "  def __init__(self, encoder, embedder):\n",
        "    self.encoder = encoder\n",
        "    self.embedder = embedder\n",
        "\n",
        "  @tf.function\n",
        "  def loss(self, X, Y, seq_length = 1):\n",
        "    states = self.initial_states\n",
        "    L = 0.0\n",
        "    for t in range(seq_length):\n",
        "      logits, states = self(X[t,:], states)\n",
        "      logits = tf.math.log(tf.nn.softmax(logits))\n",
        "      L -= logits[Y[t]]\n",
        "    return L\n",
        "\n",
        "  def fit(self, batches, val_batches, spelling_dictionary, epochs, learning_rate):\n",
        "    steps_per_epoch = batches.cardinality().numpy()\n",
        "    self.optimizer = tf.keras.optimizers.Adagrad(learning_rate = learning_rate, epsilon = 1e-8, clipvalue = 5)\n",
        "    smooth_loss = None\n",
        "    step = 0\n",
        "    for batch in batches.repeat(epochs):\n",
        "      X_batch, Y_batch = batch\n",
        "      grads_batch = []\n",
        "      n_batch = tf.shape(X_batch)[0].numpy()\n",
        "      seq_length = tf.shape(X_batch)[1].numpy()\n",
        "      for X, Y in zip(X_batch, Y_batch):\n",
        "        if smooth_loss == None:\n",
        "          smooth_loss = self.loss(X, Y, seq_length)\n",
        "        else:\n",
        "          smooth_loss = 0.999*smooth_loss + 0.001*self.loss(X, Y, seq_length)\n",
        "        with tf.GradientTape() as tape:\n",
        "          tape.watch(self.variables)\n",
        "          loss = self.loss(X, Y, seq_length)\n",
        "        grads = tape.gradient(loss, self.variables)\n",
        "        if grads_batch == []:\n",
        "          grads_batch = [g / n_batch for g in grads]\n",
        "        else:\n",
        "          for i in range(len(grads)):\n",
        "            grads_batch[i] = grads_batch[i] + grads[i]/n_batch\n",
        "      self.optimizer.apply_gradients(zip(grads_batch, self.variables))\n",
        "      if (step % steps_per_epoch == 0):\n",
        "        print(\"\\nEPOCH\", step // steps_per_epoch)\n",
        "        print(\"Smooth Loss:\", smooth_loss.numpy())\n",
        "        val_loss = 0\n",
        "        val_count = 0\n",
        "        for batch in val_batches:\n",
        "          for X, Y in zip(X_batch, Y_batch):\n",
        "            val_loss = val_loss + self.loss(X, Y, seq_length)\n",
        "            val_count = val_count + 1\n",
        "        print(\"Average validation loss:\", val_loss.numpy()/val_count)\n",
        "        text = self.generate_text_temperature('.', 200)\n",
        "        print(\"Text generation correctly spelled:\", correctly_spelled(text, spelling_dictionary))\n",
        "        print(text)\n",
        "      step = step + 1\n",
        "\n",
        "  def generate_text_temperature(self, start, length, T = 1.0):\n",
        "    text = []\n",
        "    states = self.initial_states\n",
        "    start = tf.squeeze(self.embedder(np.expand_dims(self.encoder.text_to_inds(start), axis = 0)))\n",
        "    unk_ind = self.encoder.token_to_ind[\"[UNK]\"]\n",
        "    sparse_unk_mask = tf.SparseTensor(values = [-float(\"inf\")], indices = [[unk_ind]], dense_shape=[self.encoder.get_size()])\n",
        "    for i in range(length):\n",
        "      H = states\n",
        "      logits, states = self(start, states)\n",
        "      logits = logits/T\n",
        "      logits = logits + tf.sparse.to_dense(sparse_unk_mask)\n",
        "      pred = tf.random.categorical([logits], num_samples = 1)\n",
        "      start = tf.squeeze(self.embedder(pred))\n",
        "      text.append(self.encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "    return \"\".join(text)\n",
        "\n",
        "  def generate_text_nucleus(self, start, length, theta = 1.0):\n",
        "    text = []\n",
        "    states = self.initial_states\n",
        "    start = tf.squeeze(self.embedder(np.expand_dims(self.encoder.text_to_inds(start), axis = 0)))\n",
        "    unk_ind = self.encoder.token_to_ind[\"[UNK]\"]\n",
        "    sparse_unk_mask = tf.SparseTensor(values = [-float(\"inf\")], indices = [[unk_ind]], dense_shape=[self.encoder.get_size()])\n",
        "    for i in range(length):\n",
        "      logits, states = self(start, states)\n",
        "      logits = logits + tf.sparse.to_dense(sparse_unk_mask)\n",
        "      probs = tf.nn.softmax(logits)\n",
        "      sorted_probs = tf.sort(probs, direction = \"DESCENDING\")\n",
        "      sorted_probs_sum = tf.math.cumsum(sorted_probs)\n",
        "      thresh_inds = tf.where(sorted_probs_sum <= theta)\n",
        "      if len(thresh_inds) > 0:\n",
        "        thresh_ind = thresh_inds[-1, 0].numpy()\n",
        "      else:\n",
        "        thresh_ind = 0\n",
        "      top_probs = tf.multiply(probs, tf.cast(probs >= sorted_probs[thresh_ind], \"float32\"))/sorted_probs_sum[thresh_ind]\n",
        "      pred = tf.random.categorical([tf.math.log(top_probs)], num_samples = 1)\n",
        "      start = tf.squeeze(self.embedder(pred))\n",
        "      text.append(self.encoder.ind_to_token[tf.squeeze(pred).numpy()])\n",
        "    return \"\".join(text)\n",
        "\n",
        "class RNN(Model):\n",
        "  def __init__(self, encoder, embedder, m, sig = 0.01):\n",
        "    super().__init__(encoder, embedder)\n",
        "    self.embedding_dim = embedder.output_dim\n",
        "    self.m = m\n",
        "    self.K = encoder.get_size()\n",
        "    self.b = tf.Variable(tf.zeros_initializer()(shape = (self.m)))\n",
        "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
        "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.embedding_dim)))\n",
        "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.m, self.m)))\n",
        "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
        "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
        "    self.initial_states = np.zeros(shape = (self.m), dtype = np.float32)\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, X, states):\n",
        "    H = states\n",
        "    A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X) + self.b\n",
        "    H = tf.math.tanh(A)\n",
        "    O = tf.linalg.matvec(self.V, H) + self.c\n",
        "    return O, H\n",
        "\n",
        "class LSTM(Model):\n",
        "  def __init__(self, encoder, embedder, m, sig = 0.01):\n",
        "    super().__init__(encoder, embedder)\n",
        "    self.embedding_dim = embedder.output_dim\n",
        "    self.m = m\n",
        "    self.K = encoder.get_size()\n",
        "    self.b = tf.Variable(tf.zeros_initializer()(shape = (4*self.m)))\n",
        "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
        "    self.U = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.embedding_dim)))\n",
        "    self.W = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.m)))\n",
        "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
        "    self.variables = [self.b, self.c, self.U, self.W, self.V]\n",
        "    self.initial_states = np.zeros(shape = (self.m), dtype = np.float32)\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, X, states):\n",
        "    H = states\n",
        "    A = tf.linalg.matvec(self.W, H) + tf.linalg.matvec(self.U, X) + self.b\n",
        "    f = tf.math.sigmoid(A[:self.m])\n",
        "    i = tf.math.sigmoid(A[self.m:2*self.m])\n",
        "    o = tf.math.sigmoid(A[2*self.m:3*self.m])\n",
        "    H = tf.math.tanh(A[3*self.m:])\n",
        "    O = tf.linalg.matvec(self.V, H) + self.c\n",
        "    return O, H\n",
        "\n",
        "class LSTM2(Model):\n",
        "  def __init__(self, encoder, embedder, m, sig = 0.01):\n",
        "    super().__init__(encoder, embedder)\n",
        "    self.embedding_dim = embedder.output_dim\n",
        "    self.m = m\n",
        "    self.K = encoder.get_size()\n",
        "    self.b1 = tf.Variable(tf.zeros_initializer()(shape = (4*self.m)))\n",
        "    self.b2 = tf.Variable(tf.zeros_initializer()(shape = (4*self.m)))\n",
        "    self.c = tf.Variable(tf.zeros_initializer()(shape = (self.K)))\n",
        "    self.U1 = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.embedding_dim)))\n",
        "    self.W1 = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.m)))\n",
        "    self.U2 = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.m)))\n",
        "    self.W2 = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (4*self.m, self.m)))\n",
        "    self.V = tf.Variable(tf.random_normal_initializer(mean = 0.0, stddev = sig)(shape = (self.K, self.m)))\n",
        "    self.variables = [self.b1, self.b2, self.c, self.U1, self.U2, self.W1, self.W2, self.V]\n",
        "    self.initial_states = [np.zeros(shape = (self.m), dtype = np.float32), np.zeros(shape = (self.m), dtype = np.float32)]\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, X, states):\n",
        "    H1, H2 = states\n",
        "    A1 = tf.linalg.matvec(self.W1, H1) + tf.linalg.matvec(self.U1, X) + self.b1\n",
        "    f1 = tf.math.sigmoid(A1[:self.m])\n",
        "    i1 = tf.math.sigmoid(A1[self.m:2*self.m])\n",
        "    o1 = tf.math.sigmoid(A1[2*self.m:3*self.m])\n",
        "    H1 = tf.math.tanh(A1[3*self.m:])\n",
        "    A2 = tf.linalg.matvec(self.W2, H2) + tf.linalg.matvec(self.U2, H1) + self.b2\n",
        "    f2 = tf.math.sigmoid(A2[:self.m])\n",
        "    i2 = tf.math.sigmoid(A2[self.m:2*self.m])\n",
        "    o2 = tf.math.sigmoid(A2[2*self.m:3*self.m])\n",
        "    H2 = tf.math.tanh(A2[3*self.m:])\n",
        "    O = tf.linalg.matvec(self.V, H2) + self.c\n",
        "    return O, [H1, H2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0iSGXTUDcic"
      },
      "source": [
        "Dictionary of known words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Vm-tXKuC7MTN"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  lower = text.lower()\n",
        "  no_spec = re.sub(\"\\&|\\[|\\]|\\_|!|\\?|\\*|\\.|,|\\(|\\)|;|:|[0-9]+|\\\"|\\'\",\"\", lower)\n",
        "  no_enter = re.sub(\"\\n|-\",\" \", no_spec)\n",
        "  return no_enter.split()\n",
        "\n",
        "def get_dictionary(text):\n",
        "  dictionary = {w for w in clean_text(text)}\n",
        "  return dictionary\n",
        "\n",
        "def correctly_spelled(text, dictionary):\n",
        "  count = 0\n",
        "  words = clean_text(text)\n",
        "  for w in clean_text(text):\n",
        "    if w in dictionary:\n",
        "      count += 1\n",
        "  return count/len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SlS6ITj4PQeO"
      },
      "outputs": [],
      "source": [
        "def batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, seq_length, batch_size, buffer_size, epochs, learning_rate):\n",
        "    configs = dict(\n",
        "        seq_length = seq_length,\n",
        "        batch_size = batch_size,\n",
        "        buffer_size = buffer_size,\n",
        "        K = encoder.get_size(),\n",
        "        m = model.m,\n",
        "        epochs = epochs,\n",
        "        learning_rate = learning_rate,\n",
        "    )\n",
        "    training_batches = batch_data(training_text, configs[\"seq_length\"], encoder, embedder, configs[\"batch_size\"], configs[\"buffer_size\"])\n",
        "    validation_batches = batch_data(validation_text, configs[\"seq_length\"], encoder, embedder)\n",
        "    \"\"\"wandb.init(\n",
        "            project = \"ProjectDD2424\",\n",
        "            config = configs)\n",
        "    config = wandb.config\"\"\"\n",
        "    model.fit(training_batches, validation_batches, spelling_dictionary, configs[\"epochs\"], configs[\"learning_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "haY4WnUWDcid"
      },
      "outputs": [],
      "source": [
        "books = fetch_data()\n",
        "\n",
        "training_text = books[0] #+ books[1] + books[2] + books[3] + books[4] + books[5]\n",
        "validation_text = books[6]\n",
        "test_text = books[7]\n",
        "\n",
        "training_text = training_text[1000:101000]\n",
        "validation_text = validation_text[1000:2100]\n",
        "\n",
        "basic_encoder = BasicEncoder(training_text)\n",
        "byte_pair_encoder = BytePairEncoder(training_text, 200)\n",
        "#word_encoder = WordEncoder(training_text)\n",
        "\n",
        "basic_embedder = tf.keras.layers.Embedding(basic_encoder.get_size(), basic_encoder.get_size(), embeddings_initializer = \"identity\")\n",
        "byte_pair_embedder = tf.keras.layers.Embedding(byte_pair_encoder.get_size(), byte_pair_encoder.get_size(), embeddings_initializer = \"identity\")\n",
        "#w2v_seq_length = 10\n",
        "#w2v_embedding_dim = 128\n",
        "#w2v_weights = get_w2v_weights(training_text, w2v_seq_length, word_encoder, w2v_embedding_dim, window_size = 2, n_neg_samples = 4, batch_size = 1024, buffer_size = 10000)\n",
        "#word_embedder = tf.keras.layers.Embedding(word_encoder.get_size(), w2v_embedding_dim, weights = w2v_weights, trainable = False)\n",
        "\n",
        "spelling_dictionary = get_dictionary(training_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = basic_encoder\n",
        "embedder = basic_embedder\n",
        "model = RNN(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "T2eSVC9DqBPk",
        "outputId": "e9646d16-a227-45ac-d384-4cfcd52c679b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            "    Smooth Loss: 433.0752\n",
            "    Average validation loss: 431.670654296875\n",
            "   Text generation correctly spelled: 0.0\n",
            "    !B3\"s-eVYz\n",
            ":xVRmf)QeNpG-mbDOY5UvLEt1SVttE, :;cw;nbpg8E5K,uNHtuELPod0:45Gw\n",
            "o.K5Ph G6\"V-&7;ql?!v\n",
            "An)um(dIb2arEQz0GwgRki&ZRfMuI5mqtpIrs6nq1.qT?(z v1cgc3S6btyxLRj3KN;\n",
            "HIPG;Vqr2cl!Qy (4UA;n1YO\n",
            "K;&i.aY tKAf\n",
            "EPOCH 1\n",
            "    Smooth Loss: 384.1065\n",
            "    Average validation loss: 308.538037109375\n",
            "   Text generation correctly spelled: 0.047619047619047616\n",
            "    E,hgctheiailwhh dot  -tvgfcn ,wme\n",
            "teoefesldlo,tget.lh;y; Saicaettdiqse,.daeenlee,mcgali, dnr M onh oaleocof. amktaftohietlano tIyasrst osi Ml  a \n",
            "lgilsfeu  ssnowouu eawuisn\n",
            "ca lf,n3ntolhooae\n",
            "aeunoec  \n",
            "EPOCH 2\n",
            "    Smooth Loss: 337.8351\n",
            "    Average validation loss: 313.1955810546875\n",
            "   Text generation correctly spelled: 0.03333333333333333\n",
            "    )nuogea e l  wl dbao hn .lsht\"  uye \n",
            "oelnogw lsmmeehfmleoatsoot.rc ew rs ealein iot,btysi.d h, rae  uyemgeolteW  ai e mpa0adulbuiunhpar  eSk ilnead oaeesuerpete f aaehieenseon yt dnv\n",
            "hceneo  a hlolnvr\n",
            "EPOCH 3\n",
            "    Smooth Loss: 319.21637\n",
            "    Average validation loss: 308.2697509765625\n",
            "   Text generation correctly spelled: 0.10526315789473684\n",
            "    7qtvsce pl.h w\n",
            "n onhse r udnh e gni e t;ee a lfr tih toe oty teyrWowliinrtot asnmasav\n",
            "he a s\n",
            "tod\"gfv aatane\n",
            "tao  r lo iepovnoth i leaitawsHp t,oosle oosieh h,gh tsibfoaeltthafe stgblwi    ne  nTd te l\n",
            "EPOCH 4\n",
            "    Smooth Loss: 311.89032\n",
            "    Average validation loss: 306.0388427734375\n",
            "   Text generation correctly spelled: 0.06976744186046512\n",
            "    vg-g qsio dnnir,C tnl ea,lw\n",
            "a,:\n",
            "gestsn t  ng,rs o l atto n t k Snatsa alo onaier xtrsmgnve  og n lien\"nosah  u  on oIeae ohoeasoh f i'm t hd dboar \n",
            " tegsr yn l eflrrhvor  ite  gh-etrLmhkc   o or\n",
            "onn f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = basic_encoder\n",
        "embedder = basic_embedder\n",
        "model = LSTM(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "JicXrOTt7eoU",
        "outputId": "0dedbd13-3e58-4f4b-907a-9e246b2ac3d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            "    Smooth Loss: 433.06848\n",
            "    Average validation loss: 431.6236328125\n",
            "   Text generation correctly spelled: 0.0\n",
            "    yvkM4n.S?W9?:Van7iTENF:D1WqU5O3v.vSNDenylLQeqHs.HDBHu9PCfpqO3zA)4a0B390tCj)EUA2ykCTJ6yVp'A\n",
            "njbiHGPIMxTM0QaZl0NtQd;uRIt(1BJW7Y\"n;tDQ,)WdxLv0;gdtDA:rfF'Jb,FdpCkaWx,V.Zlh6,;'jNgkUDNMeQ.\n",
            "zOlTLFoj.?jG-H8gz\n",
            "EPOCH 1\n",
            "    Smooth Loss: 382.84\n",
            "    Average validation loss: 309.5887451171875\n",
            "   Text generation correctly spelled: 0.1111111111111111\n",
            "    CSsecnmceheent g jdr n  et,fetao fure ia \n",
            ", ielaeflonw gnot y yns  as i dnc-ddetra wlraw hneiyrt\n",
            "tfb,wi  n ,osi g,tao nye\n",
            "frH,al   re are tlsidgl eete  ntrrrh;oE aoaro\"3 cbpv b fgdgd\n",
            "Noni hs  mor cana\n",
            "EPOCH 2\n",
            "    Smooth Loss: 336.96198\n",
            "    Average validation loss: 307.58427734375\n",
            "   Text generation correctly spelled: 0.038461538461538464\n",
            "    :mcsoRah flhhsaaooarl\n",
            "irlatf gaitu ivgdmpdre.h\n",
            "hel a elyw\" eeoepsowatlhaghtine\n",
            "teftper lei mstdyd\"aaefe n,y irnnhsdsan, \"le \n",
            "neg,nk yel oytom glr ra ifoea t irtohthrtenghristt,eZwawows ifna;r ,  lhe t\n",
            "EPOCH 3\n",
            "    Smooth Loss: 319.32068\n",
            "    Average validation loss: 309.57587890625\n",
            "   Text generation correctly spelled: 0.030303030303030304\n",
            "    -Kotei ae g.reyaamom eri rhf uosvrf ha R  auastosssw l srgwlaeld,gs s mso ptnlew, h nnogtehn  \"iv  vshsl toeh,os  itfdi h\n",
            "q Aeiyenheete sla hg eWlg;n\n",
            "it  rllrtgs respeehoao wle osvrmemAmtfmdtc\n",
            " n lnag\n",
            "EPOCH 4\n",
            "    Smooth Loss: 311.9186\n",
            "    Average validation loss: 305.509130859375\n",
            "   Text generation correctly spelled: 0.05\n",
            "    6 j\n",
            " hl te ecee\n",
            "mevhth  \n",
            "mqhh adtn .e erm nr o weavl  1 hopyrht sslvyon ehiMtfrlify ,toycgn nt i\n",
            "celd k ao dxma ln ntoweiaio mnten\n",
            "shrieoi r  hhhn tqv \n",
            "ingi\n",
            " lhl othdri g\n",
            "wvr\"d m  ecrt\n",
            "elttie\n",
            "\n",
            "e nt st\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = basic_encoder\n",
        "embedder = basic_embedder\n",
        "model = LSTM2(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "Zb73285b7ifL",
        "outputId": "84ce480e-9197-48b8-9ca5-761cd596f786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            "    Smooth Loss: 433.0756\n",
            "    Average validation loss: 431.66845703125\n",
            "   Text generation correctly spelled: 0.0\n",
            "    O?'sjb9Ee5LYPrO3uC)AWbI?(v9i;h0wIl4YaFD79cW(DDqLlY8ks7D1('LNc\n",
            "KDS:J-Lz\n",
            "Gw:NnMpLUevfUbF0rUws5ZKejlMOkofR26O7PDy7Ysk,j94P74vq;U:3c(9:C.?N8Hc2mA-h9\n",
            "a!kR\"hOx\"\n",
            "47YHL5Q;Hme-ph APN: Fnof)WBa!-llYTi49.E;&RTC \n",
            "EPOCH 1\n",
            "    Smooth Loss: 386.55298\n",
            "    Average validation loss: 307.343359375\n",
            "   Text generation correctly spelled: 0.0\n",
            "    ;hoE sawafloa b  ooVn uis,odeh Rbuhhaehti nwmeIoi  gywgahsaua  Eemaeifselho r6unih ray dthlrtV\n",
            "e enrev lnheeoaset olrttu  elfoftt dh ddmawSsemd oss msbntlderhese fs\n",
            "omctslbf  rnlleoem lBipgsehgv ue,di\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bae30755a6eb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspelling_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-47be044b214e>\u001b[0m in \u001b[0;36mbatch_and_train\u001b[0;34m(model, encoder, embedder, spelling_dictionary, training_text, validation_text, seq_length, batch_size, buffer_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m             config = configs)\n\u001b[1;32m     16\u001b[0m     config = wandb.config\"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspelling_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-5ac74a69a867>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, batches, val_batches, spelling_dictionary, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmooth_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = byte_pair_encoder\n",
        "embedder = byte_pair_embedder\n",
        "model = RNN(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "aV4XmhBn7meL",
        "outputId": "1c2fbd6d-b039-4c02-98d8-55c78d89193c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'super' has no attribute 'text_to_inds'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e03d74e3d0e0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte_pair_embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspelling_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-47be044b214e>\u001b[0m in \u001b[0;36mbatch_and_train\u001b[0;34m(model, encoder, embedder, spelling_dictionary, training_text, validation_text, seq_length, batch_size, buffer_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtraining_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seq_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"buffer_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seq_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"wandb.init(\n",
            "\u001b[0;32m<ipython-input-5-0dca5e9c8f9c>\u001b[0m in \u001b[0;36mbatch_data\u001b[0;34m(text, seq_length, encoder, embedder, batch_size, buffer_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ded4ab2a94ce>\u001b[0m in \u001b[0;36mtext_to_inds\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mfound_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mfound_merge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'super' has no attribute 'text_to_inds'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = byte_pair_encoder\n",
        "embedder = byte_pair_embedder\n",
        "model = LSTM(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "4SpIlIdt7qpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = byte_pair_encoder\n",
        "embedder = byte_pair_embedder\n",
        "model = LSTM2(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 5, 0.01)"
      ],
      "metadata": {
        "id": "P88O68rI7tRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = word_encoder\n",
        "embedder = word_embedder\n",
        "model = RNN(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 10, 0.01)"
      ],
      "metadata": {
        "id": "ElzWFz3k8Bvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = word_encoder\n",
        "embedder = word_embedder\n",
        "model = LSTM(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 10, 0.01)"
      ],
      "metadata": {
        "id": "EbSRi4hY8Jbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = word_encoder\n",
        "embedder = word_embedder\n",
        "model = LSTM2(encoder, embedder, 100)\n",
        "batch_and_train(model, encoder, embedder, spelling_dictionary, training_text, validation_text, 100, 64, 10000, 10, 0.01)"
      ],
      "metadata": {
        "id": "Ib6O7gAk8J7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}